{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58272c51",
   "metadata": {},
   "source": [
    "\n",
    "# ETH Foundation Discursive Analysis — Master Pipeline\n",
    "\n",
    "Ce notebook exécute l'ensemble du pipeline critique sur le corpus Ethereum Foundation (567 articles).\n",
    "\n",
    "**Modules inclus :**\n",
    "1. Extraction de fréquence\n",
    "2. Visualisation des fréquences\n",
    "3. Cooccurrences\n",
    "4. Réseau lexical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paramètres globaux de base\n",
    "DATA_DIR = './data'\n",
    "OUTPUT_DIR = './outputs'\n",
    "\n",
    "# Création des dossiers si nécessaires\n",
    "import os\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24bd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Stopwords simples à améliorer au besoin\n",
    "STOPWORDS = set([\n",
    "    'the', 'and', 'of', 'to', 'in', 'for', 'is', 'on', 'that', 'with', 'as',\n",
    "    'by', 'this', 'it', 'are', 'at', 'from', 'an', 'be', 'or', 'we', 'can',\n",
    "    'not', 'have', 'has', 'our', 'also', 'more', 'which', 'their', 'will',\n",
    "    'all', 'but', 'was', 'they', 'these', 'may', 'you', 'been', 'using', 'its'\n",
    "])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in STOPWORDS and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Extraction fréquence\n",
    "all_tokens = []\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(DATA_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            clean = clean_text(content)\n",
    "            tokens = tokenize(clean)\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "total_tokens = sum(counter.values())\n",
    "data = [{'word': word, 'frequency': freq, 'relative_per_1000': freq/total_tokens*1000} for word, freq in counter.most_common()]\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f'{OUTPUT_DIR}/word_frequencies.csv', index=False)\n",
    "df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e48881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df = pd.read_csv(f'{OUTPUT_DIR}/word_frequencies.csv')\n",
    "N = 30\n",
    "top_words = df.head(N)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_words['word'], top_words['frequency'], color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(f'Top {N} mots les plus fréquents')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/top_words_bar_chart.png')\n",
    "plt.show()\n",
    "\n",
    "word_freq = dict(zip(df['word'], df['frequency']))\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Nuage de mots complet')\n",
    "plt.savefig(f'{OUTPUT_DIR}/wordcloud.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "cooccurrence = defaultdict(int)\n",
    "\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(DATA_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            clean = clean_text(content)\n",
    "            tokens = tokenize(clean)\n",
    "            for i in range(len(tokens) - WINDOW_SIZE + 1):\n",
    "                window = tokens[i:i+WINDOW_SIZE]\n",
    "                for w1, w2 in combinations(set(window), 2):\n",
    "                    pair = tuple(sorted((w1, w2)))\n",
    "                    cooccurrence[pair] += 1\n",
    "\n",
    "data_pairs = [{'word1': p[0], 'word2': p[1], 'count': c} for p, c in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)]\n",
    "df_pairs = pd.DataFrame(data_pairs)\n",
    "df_pairs.to_csv(f'{OUTPUT_DIR}/cooccurrence_pairs.csv', index=False)\n",
    "df_pairs.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926390e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "\n",
    "df_pairs = pd.read_csv(f'{OUTPUT_DIR}/cooccurrence_pairs.csv')\n",
    "threshold = 5  # seuil minimal pour épuration visuelle\n",
    "df_filtered = df_pairs[df_pairs['count'] >= threshold]\n",
    "G = nx.Graph()\n",
    "\n",
    "for index, row in df_filtered.iterrows():\n",
    "    G.add_edge(row['word1'], row['word2'], weight=row['count'])\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=50)\n",
    "edges = G.edges(data=True)\n",
    "weights = [edata['weight'] for _,_,edata in edges]\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=300, node_color='skyblue')\n",
    "nx.draw_networkx_edges(G, pos, width=[w/2 for w in weights], alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
    "\n",
    "plt.title(\"Network of lexical co-occurrences\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/lexical_network.png')\n",
    "plt.show()\n",
    "\n",
    "nx.write_graphml(G, f\"{OUTPUT_DIR}/lexical_network.graphml\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
