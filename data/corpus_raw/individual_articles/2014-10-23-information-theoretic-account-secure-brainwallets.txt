Titre: An Information-Theoretic Account of Secure Brainwallets\nAuteur: Vitalik Buterin\nDate: October 23, 2014\nURL: https://blog.ethereum.org/2014/10/23/information-theoretic-account-secure-brainwallets\nCatégorie: Non catégorisé\n\n==================================================\n\nn important and controversial topic in the area of personal wallet security is the concept of "brainwallets" - storing funds using a private key generated from a password memorized entirely in one's head. heoretically, brainwallets have the potential to provide almost utopian guarantee of security for long-term savings for as long as they are kept unused, they are not vulnerable to physical theft or hacks of any kind, and there is no way to even prove that you still remember the wallet they are as safe as your very own human mind. t the same time, however, many have argued against the use of brainwallets, claiming that the human mind is fragile and not well designed for producing, or remembering, long and fragile cryptographic secrets, and so they are too dangerous to work in reality. hich side is right s our memory sufficiently robust to protect our private keys, is it too weak, or is perhaps a third and more interesting possibility actually the case that it all depends on how the brainwallets are producednnntropynnf the challenge at hand is to create a brainwallet that is simultaneously memorable and secure, then there are two variables that we need to worry about how much information we have to remember, and how long the password takes for an attacker to crack. s it turns out, the challenge in the problem lies in the fact that the two variables are very highly correlated in fact, absent a few certain specific kinds of special tricks and assuming an attacker running an optimal algorithm, they are precisely equivalent (or rather, one is precisely eponential in the other). owever, to start off we can tackle the two sides of the problem separately.nn common measure that computer scientists, cryptogaphers and mathematicians use to measure "how much information" a piece of data contains is "entropy". oosely defined, entropy is defined as the logarithm of the number of possible messages that are of the same "form" as a given message. or eample, consider the number .  seems to be in the category of five-digit numbers, of which there are . ence, the number contains about . bits of entropy, as nn.nn~ . he number  is  digits long, and log(nnnn) ~ ., so it has . bits of entropy.  random string of ones and zeroes n bits long will contain eactly n bits of entropy. hus, longer strings have more entropy, and strings that have more symbols to choose from have more entropy.nnn the other hand, the number  has much less than . bits of entropy although it has  digits, the number is not of the category of -digit numbers, it is in the category of -digit numbers with a very high level of structure a complete list of numbers with at least that level of structure might be at most a few billion entries long, giving it perhaps only  bits of entropy.nnnformation theory has a number of more formal definitions that try to grasp this intuitive concept.  particularly popular one is the idea of olmogorov compleity the olmogorov compleity of a string is basically the length of the shortest computer program that will print that value. n ython, the above string is also epressible asnn''*+''nn- an -character string, while  takes  characters (the actual digits plus quotes). his gives us a more formal understanding of the idea of "category of strings with high structure" - those strings are simply the set of strings that take a small amount of data to epress. ote that there are other compression strategies we can use for eample, unbalanced strings like  can be cut by at least half by creating special symbols that represent multiple s in sequence.nnuffman codingnnis an eample of an information-theoretically optimal algorithm for creating such transformations.nninally, note that entropy is contet-dependent. he string "the quick brown fo jumped over the lazy dog" may have over  bytes of entropy as a simple uffman-coded sequence of characters, but because we know nglish, and because so many thousands of information theory articles and papers have already used that eact phrase, the actual entropy is perhaps around  bytes -  might refer to it as "fo dog phrase" andnnusing ooglennyou can figure out what it is.nno what is the point of entropy ssentially, entropy is how much information you have to memorize. he more entropy it has, the harder to memorize it is. hus, at first glance it seems that you want passwords that are as low-entropy as possible, while at the same time being hard to crack. owever, as we will see below this way of thinking is rather dangerous.nntrengthnnow, let us get to the net point, password security against attackers. he security of a password is best measured by the epected number of computational steps that it would take for an attacker to guess your password. or randomly generated passwords, the simplest algorithm to use is brute force try all possible one-character passwords, then all two-character passwords, and so forth. iven an alphabet of n characters and a password of length k, such an algorithm would crack the password in roughly nnnknntime. ence, the more characters you use, the better, and the longer your password is, the better.nnhere is one approach that tries to elegantly combine these two strategies without being too hard to memorize teve ibson'snnhaystack passwordsnn. s teve ibson eplainsnnhich of the following two passwords is stronger, more secure, and more difficult to cracknng.....................nnryc.(nk#!edfpnnou probably know this is a trick question, but the answer is espite the fact that the first password is  easier to use and more memorable, it is also the stronger of the two! n fact, since it is one character longer and contains uppercase, lowercase, a number and special characters, that first password would take an attacker approimately  times longer to find by searching than the second impossible-to-remember-or-type password!nnteve then goes on to write "irtually everyone has always believed or been told that passwords derived their strength from having “high entropy”. ut as we see now, when the only available attack is guessing, that long-standing common wisdom  . . . is  . . . not  . . . correct!" owever, as seductive as such a loophole is, unfortunately in this regard he is dead wrong. he reason is that it relies on specific properties of attacks that are commonly in use, and if it becomes widely used attacks could easily emerge that are specialized against it. n fact, there is a generalized attack that, given enough leaked password samples, cannnautomatically update itself to handle almost anythingnnnnarkov chainnnsamplers.nnhe way the algorithm works is as follows. uppose that the alphabet that you have consists only of the characters  and , and you know from sampling that a  is followed by a  % of the time and a  % of the time, and a  is followed by a  % of the time and a  % of the time. o randomly sample the set, we create a finite state machine containing these probabilities, and simply run it over and over again in a loop.nnere's the ython codennimportnnrandomnninnnnnnwhilennnnnnifnninnnnnnnninnnnnnifnnrandom.randrangenn(nnnn)nnnnnnelsennnnelifnninnnnnnnninnnnnnifnnrandom.randrangenn(nnnn)nnnnnnelsennnnprint inne take the output, break it up into pieces, and there we have a way of generating passwords that have the same pattern as passwords that people actually use. e can generalize this past two characters to a complete alphabet, and we can even have the state keep track not just of the last character but the last two, or three or more. o if everyone starts making passwords like "g.....................", then after seeing a few thousand eamples the arkov chain will "learn" that people often make long strings of periods, and if it spits out a period it will often get itself temporarily stuck in a loop of printing out more periods for a few steps - probabilistically replicating people's behavior.nnhe one part that was left out is how to terminate the loop as given, the code simply gives an infinite string of zeroes and ones. e could introduce a pseudo-symbol into our alphabet to represent the end of a string, and incorporate the observed rate of occurrences of that symbol into our arkov chain probabilities, but that's not optimal for this use case - because far more passwords are short than long, it would usually output passwords that are very short, and so it would repeat the short passwords millions of times before trying most of the long ones. hus we might want to artificially cut it off at some length, and increase that length over time, although more advanced strategies also eist like running a simultaneous arkov chain backwards. his general category of method is usually called a "nnlanguage modelnn" - a probability distribution over sequences of characters or words which can be as simple and rough or as comple and intricate as needed, and which can then be sampled.nnhe fundamental reason why the ibson strategy fails, and why no other strategy of that kind can possibly work, is that in the definitions of entropy and strength there is an interesting equivalence entropy is the logarithm of the number of possibilities, but strength is the number of possibilities - in short, memorizability and attackability are invariably eactly the same! his applies regardless of whether you are randomly selecting characters from an alphabet, words from a dictionary, characters from a biased alphabet (eg. "" % of the time and "" % of the time, or strings that follow a particular pattern). hus, it seems that the quest for a secure and memorizable password is hopeless...nnasing emory, ardening ttacksnn... or not. lthough the basic idea that entropy that needs to be memorized and the space that an attacker needs to burn through are eactly the same is mathematically and computationally correct, the problem lives in the real world, and in the real world there are a number of compleities that we can eploit to shift the equation to our advantage.nnhe first important point is that human memory is not a computer-like store of data the etent to which you can accurately remember information often depends on how you memorize it, and in what format you store it. or eample, we implicitly memorize kilobytes of information fairly easily in the form of human faces, but even something as similar in the grand scheme of things as dog faces are much harder for us. nformation in the form of tet is even harder - although if we memorize the tet visually and orally at the same time it's somewhat easier again.nnome have tried to take advantage of this fact by generating random brainwallets and encoding them in a sequence of words for eample, one might see something likennwitch collapse practice feed shamennopennndespair creek road again ice leastnnnnpopular  comicnnillustrates the principle, suggesting that users create passwords by generating four random words instead of trying to be clever with symbol manipulation. he approach seems elegant, and perhaps taking away of our differing ability to remember random symbols and language in this way, it just might work. cept, there's a problem it doesn't.nno quote annrecent study by ichard hay and othersnnfrom arnegie ellonnnn a ,-participant online study, we eplored the usability of - and -word system- assigned passphrases in comparison to system-assigned passwords composed of  to  random characters, and -character system-assigned pronounceable passwords. ontrary to epectations, sys- tem-assigned passphrases performed similarly to system-assigned passwords of similar entropy across the usability metrics we e- amined. assphrases and passwords were forgotten at similar rates, led to similar levels of user difficulty and annoyance, and were both written down by a majority of participants. owever, passphrases took significantly longer for participants to enter, and appear to require error-correction to counteract entry mistakes. assphrase usability did not seem to increase when we shrunk the dictionary from which words were chosen, reduced the number of words in a passphrase, or allowed users to change the order of words.nnowever, the paper does leave off on a note of hope. t does note that there are ways to make passwords that are higher entropy, and thus higher security, while still being just as easy to memorize randomly generated but pronounceable strings like "zelactudet" (presumably created via some kind of per-character language model sampling) seem to provide a moderate gain over both word lists and randomly generated character strings.  likely cause of this is that pronounceable passwords are likely to be memorized both as a sound and as a sequence of letters, increasing redundancy. hus, we have at least one strategy for improving memorizability without sacrificing strength.nnhe other strategy is to attack the problem from the opposite end make it harder to crack the password without increasing entropy. e cannot make the password harder to crack by adding more combinations, as that would increase entropy, but what we can do is use what is known as a hardnnkey derivation functionnn. or eample, suppose that if our memorized brainwallet isnnbnn, instead of making the private keynnsha(b)nnornnsha(b)nn, we make itnn(b, )nnwherennnnis defined as followsnndef nn(nnb, roundsnn)nnnnnnnnbnninnnnnnwhilenninnnnroundsnnnnnnshann(nn + bnn)nninn+nnnnreturnnnnnssentially, we keep feedingnnbnninto the hash function over and over again, and only after  rounds do we take the output.nneeding the original input back into each round is not strictly necessary, but cryptographers recommend it in order to limit the effect of attacks involving precomputednnrainbow tablesnn. ow, checking each individual password takes a thousand time longer. ou, as the legitimate user, won't notice the difference - it's  milliseconds instead of  microseconds - but against attackers you get ten bits of entropy for free, without having to memorize anything more. f you go up to  rounds you get fifteen bits of entropy, but then calculating the password takes close to a second  bits takes  seconds, and beyond about  it becomes too long to be practical.nnow, there is one clever way we can go even furthernnoutsourceable ultra-epensive snn. he idea is to come up with a function which is etremely epensive to compute (eg. nnnncomputational steps), but which can be computed in some way without giving the entity computing the function access to the output. he cleanest, but most cryptographically complicated, way of doing this is to have a function which can somehow be "blinded" sonnunblind((blind()))  ()nnand blinding and unblinding requires a one-time randomly generated secret. ou then calculatennblind(password)nn, and ship the work off to a third party, ideally with an , and then unblind the response when you receive it.nnne eample of this is using elliptic curve cryptography generate a weak curve where the values are only  bits long instead of , and make the hard problem a discrete logarithm computation. hat is, we calculate a valuennnnby taking the hash of a value, find the associatednnynnon the curve, then we "blind" thenn(,y)nnpoint by adding another randomly generated point,nnnn(whose associated private key we know to bennnnn), and then ship the result off to a server to crack. nce the server comes up with the private key corresponding tonn + (,y)nn, we subtractnnnnn, and we get the private key corresponding tonn(,y)nn- our intended result. he server does not learn any information about what this value, or evennn(,y)nn, is - theoretically it could be anything with the right blinding factornnnn. lso, note that the user can instantly verify the work - simply convert the private key you get back into a point, and make sure that the point is actuallynn(,y)nn.nnnother approach relies somewhat less on algebraic features of nonstandard and deliberately weak elliptic curves use hashes to derive  seeds from a password, apply a very hard proof of work problem to each one (eg. calculatennf(h)  nnnwherennnnnis such thatnnsha(n+h)  ^nn), and combine the values using a moderately hard  at the end. nless all  servers collude (which can be avoided if the user connects through or, since it would be impossible even for an attacker controlling or seeing the results of % of the network to determine which requests are coming from the same user), the protocol is secure.nnhe interesting thing about both of these protocols is that they are fairly easy to turn into a "useful proof of work" consensus algorithm for a blockchain anyone could submit work for the chain to process, the chain would perform the computations, and both elliptic curve discrete logs and hash-based proofs of work are very easy to verify. he elegant part of the scheme is that it turns to social use both users' epenses in computing the work function, but also attackers' much greater epenses. f the blockchain subsidized the proof of work, then it would be optimal for attackers to also try to crack users' passwords by submitting work to the blockchain, in which case the attackers would contribute to the consensus security in the process. ut then, in reality at this level of security, where nnnnwork is needed to compute a single password, brainwallets and other passwords would be so secure that no one would even bother attacking them.nnntropy ifferentialsnnow, we get to our final, and most interesting, memorization strategy. rom what we discussed above, we know that entropy, the amount of information in a message, and the compleity of attack are eactly identical - unless you make the process deliberately slower with epensive s. owever, there is another point about entropy that was mentioned in passing, and which is actually crucial eperienced entropy is contet-dependent. he name "ahmoud hmadjinejad" might have perhaps ten to fifteen bits of entropy to us, but to someone living in ran while he was president it might have only four bits - in the list of the most important people in their lives, he is quite likely in the top siteen. our parents or spouse are completely unknown to myself, and so for me their names have perhaps twenty bits of entropy, but to you they have only two or three bits.nnhy does this happen ormally, the best way to think about it is that for each person the prior eperiences of their lives create a kind of compression algorithm, and under different compression algorithms, or different programming languages, the same string can have a different olmogorov compleity. n ython, '' is justnn''*nn, but in avascript it'snnrray().join("")nn. n a hypothetical version of ython with the variablennnnpreset to '', it's justnnnn. he last eample, although seemingly contrived, is actually the one that best describes much of the real world the human mind is a machine with many variables preset by our past eperiences.nnhis rather simple insight leads to a particularly elegant strategy for password memorizability try to create a password where the "entropy differential", the difference between the entropy to you and the entropy to other people, is as large as possible. ne simple strategy is to prepend your own username to the password. f my password were to be "yui&(_",  might do "vbuterinyui&(_" instead. y username might have about ten to fifteen bits of entropy to the rest of the world, but to me it's almost a single bit. his is essentially the primary reason why usernames eist as an account protection mechanism alongside passwords even in cases where the concept of users having "names" is not strictly necessary.nnow, we can go a bit further. ne common piece of advice that is now commonly and universally derided as worthless is to pick a password by taking a phrase out of a book or song. he reason why this idea is seductive is because it seems to cleverly eploit differentials the phrase might have over  bits of entropy, but you only need to remember the book and the page and line number. he problem is, of course, that everyone else has access to the books as well, and they can simply do a brute force attack over all books, songs and movies using that information.nnowever, the advice is not worthless in fact, if used as onlynnpartnnof your password, a quote from a book, song or movie is an ecellent ingredient. hy imple it creates a differential. our favorite line from your favorite song only has a few bits of entropy to you, but it's not everyone's favorite song, so to the entire world it might have ten or twenty bits of entropy. he optimal strategy is thus to pick a book or song that you really like, but which is also maimally obscure - push your entropy down, and others' entropy higher. nd then, of course, prepend your username and append some random characters (perhaps even a random pronounceable "word" like "zelactudet"), and use a secure .nnonclusionnnow much entropy do you need to be secure ight now, password cracking chips can perform aboutnnnnnnattempts per secondnn, and itcoin miners can perform roughly nnnnhashes per second (that's  terahash). he entire itcoin network together doesnn petahashesnn, or about nnnnhashes per second. ryptographers generally consider nnnnto be an acceptable minimum level of security. o get  bits of entropy, you need either about  random letters of the alphabet, or  random letters, numbers and symbols. owever, we can shave quite a bit off the requirement fifteen bits for a username, fifteen bits for a good , perhaps ten bits for an abbreviation from a passage from a semi-obscure song or book that you like, and then  more bits of plan old simple randomness. f you're not using a good , then feel free to use other ingredients.nnt has become rather popular among security eperts to dismiss passwords as being fundamentally insecure, and argue for password schemes to be replaced outright.  common argument is that because of oore's law attackers' power increases by one bit of entropy every two years, so you will have to keep on memorizing more and more to remain secure. owever, this is not quite correct. f you use a hard , oore's law allows you to take away bits from the attacker's power just as quickly as the attacker gains power, and the fact that schemes such as those described above, with the eception of s (the moderate kind, not the outsourceable kind), have not even been tried suggests that there is still some way to go. n the whole, passwords thus remain as secure as they have ever been, and remain very useful as one ingredient of a strong security policy - just not the only ingredient. oderate approaches that use a combination of hardware wallets, trusted third parties and brainwallets may even be what wins out in the end.