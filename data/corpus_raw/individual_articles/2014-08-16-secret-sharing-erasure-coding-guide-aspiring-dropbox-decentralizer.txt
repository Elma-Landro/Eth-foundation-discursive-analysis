Titre: Secret Sharing and Erasure Coding: A Guide for the Aspiring Dropbox Decentralizer\nAuteur: Vitalik Buterin\nDate: August 16, 2014\nURL: https://blog.ethereum.org/2014/08/16/secret-sharing-erasure-coding-guide-aspiring-dropbox-decentralizer\nCatégorie: Non catégorisé\n\n==================================================\n\nne of the more eciting applications of decentralized computing that have aroused a considerable amount of interest in the past year is the concept of an incentivized decentralized online file storage system. urrently, if you want your files or data securely backed up "in the cloud", you have three choices - () upload them to your own servers, () use a centralized service like oogle rive or ropbo or () use an eisting decentralized file system like reenet. hese approaches all have their own faults the first has a high setup and maintenance cost, the second relies on a single trusted party and often involves heavy price markups, and the third is slow and very limited in the amount of space that it allows each user because it relies on users to volunteer storage. ncentivized file storage protocols have the potential to provide a fourth way, providing a much higher quantity of storage and quality of service by incentivizing actors to participate without introducing centralization.nn number of platforms, includingnntornn,nnaidsafenn, to some etentnnermacoinnn, andnnilecoinnn, are attempting to tackle this problem, and the problem seems simple in the sense that all the tools are either already there or en route to being built, and all we need is the implementation. owever, there is one part of the problem that is particularly important how do we properly introduce redundancy edundancy is crucial to security especially in a decentralized network that will be highly populated by amateur and casual users, we absolutely cannot rely on any single node to stay online. e could simply replicate the data, having a few nodes each store a separate copy, but the question is can we do better s it turns out, we absolutely can.nnerkle rees and hallenge-esponse rotocolsnnefore we get into the nitty gritty of redundancy, we will first cover the easier part how do we create at least a basic system that will incentivize at least one party to hold onto a file ithout incentivization, the problem is easy you simply upload the file, wait for other users to download it, and then when you need it again you can make a request querying for the file by hash. f we want to introduce incentivization, the problem becomes somewhat harder - but, in the grand scheme of things, still not too hard.nnn the contet of file storage, there are two kinds of activities that you can incentivize. he first is the actual act of sending the file over to you when you request it. his is easy to do the best strategy is a simple tit-for-tat game where the sender sends over  kilobytes, you send over . coins, the sender sends over another  kilobytes, etc. ote that for very large files without redundancy this strategy is vulnerable to etortion attacks - quite often, .% of a file is useless to you without the last .%, so the storer has the opportunity to etort you by asking for a very high payout for the last block. he cleverest fi to this problem is actually to make the file itself redundant, using a special kind of encoding to epand the file by, say, .% so that any % of this etended file can be used to recover the original, and then hiding the eact redundancy percentage from the storer however, as it turns out we will discuss an algorithm very similar to this for a different purpose later, so for now, simply accept that this problem has been solved.nnhe second act that we can incentivize is the act of holding onto the file and storing it for the long term. his problem is somewhat harder - how can you prove that you are storing a file without actually transferring the whole thing ortunately, there is a solution that is not too difficult to implement, using what has now hopefully established a familiar reputation as the cryptoeconomist's best friend erkle trees.nnell, atricia erkle might be better in some cases, to be precise. though here the plain old original erkle will do.nnhe basic approach is this. irst, split the file up into very small chunks, perhaps somewhere between  and  bytes each, and add chunks of zeroes until the number of chunks reachesnnnnnnnnn^knnfor somennknn(the padding step is avoidable, but it makes the algorithm simpler to code and eplain). hen, we build the tree. ename thennnnnchunks that we receivednnchunknnnnnnn]nntonnchunknnnnn-nn]nn, and then rebuild chunksnnnntonnn-nnwith the following rulennchunknnnninn]nnnnshann(nnnnchunknnnnnn*inn]nn, chunknnnnnn*i+nn]nn]nn)nn. his lets you calculate chunksnnn/nntonnn-nn, thennnn/nntonnn/ -nnnn, and so forth going up the tree until there is one "root",nnchunknnnnnn]nn.nnow, note that if you store only the root, and forget aboutnnchunk] ... chunkn-]nn, the entity storing those other chunks can prove to you that they have any particular chunk with only a few hundred bytes of data. he algorithm is relatively simple. irst, we define a functionnnpartner(n)nnwhich givesnnn-nnifnnnnnis odd, otherwisennn+nn- in short, given a chunk find the chunk that it is hashed together with in order to produce the parent chunk. hen, if you want to prove ownership ofnnchunkk]nnwithnnn  k  n-nn(ie. any part of the original file), submitnnchunkpartner(k)]nn,nnchunkpartner(k/)]nn(division here is assumed to round down, so eg.nn /   nn),nnchunkpartner(k/)]nnand so on down tonnchunk]nn, alongside the actualnnchunkk]nn. ssentially, we're providing the entire "branch" of the tree going up from that node all the way to the root. he verifier will then takennchunkk]nnandnnchunkpartner(k)]nnand use that to rebuildnnchunkk/]nn, use that andnnchunkpartner(k/)]nnto rebuildnnchunkk/]nnand so forth until the verifier gets tonnchunk]nn, the root of the tree. f the root matches, then the proof is fine otherwise it's not.nnhe proof of chunk  includes () chunk , and () chunks  (nnnnnnpartnernn(nnnn)nn),  (nnnnnnpartnernn(nnnn/nn)nn) and  (nnnnnnpartnernn(nnnn/nn)nn). he verification process involves starting off with chunk , using each partner chunk in turn to recompute first chunk , then chunk , then chunk , and seeing if chunk  matches the value that the verifier had already stored as the root of the file.
ote that the proof implicitly includes the inde - sometimes you need to add the partner chunk on the right before hashing and sometimes on the left, and if the inde used to verify the proof is different then the proof will not match. hus, if  ask for a proof of piece , and you instead provide even a valid proof of piece ,  will notice that something is wrong. lso, there is no way to provide a proof without ownership of the entire relevant section of the erkle tree if you try to pass off fake data, at some point the hashes will mismatch and the final root will be different.nnow, let's go over the protocol.  construct a erkle tree out of the file as described above, and upload this to some party. hen, every  hours,  pick a random number innn, ^k-]nnand submit that number as a challenge. f the storer replies back with a erkle tree proof, then  verify the proof and if it is correct send .  (or , or storjcoin, or whatever other token is used). f  receive no proof or an invalid proof, then  do not send . f the storer stores the entire file, they will succeed % of the time, if they store % of the file they will succeed % of the time, etc. f we want to make it all-or-nothing, then we can simply require the storer to solve ten consecutive proofs in order to get a reward. he storer can still get away with storing %, but then we take advantage of the same redundant coding strategy that  mentioned above and will describe below to make % of the file sufficient in any case.nnne concern that you may have at this point is privacy - if you use a cryptographic protocol to let any node get paid for storing your file, would that not mean that your files are spread around the internet so that anyone can potentially access them ortunately the answer to this is simple encrypt the file before sending it out. rom this point on, we'll assume that all data is encrypted, and ignore privacy because the presence of encryption resolves that issue almost completely (the "almost" being that the size of the file, and the times at which you access the file, are still public).nnooking to ecentralizenno now we have a protocol for paying people to store your data the algorithm can even be made trust-free by putting it into an thereum contract, usingnnblock.prevhashnnas a source of random data to generate the challenges. ow let's go to the net step figuring out how to decentralize the storage and add redundancy. he simplest way to decentralize is simple replication instead of one node storing one copy of the file, we can have five nodes storing one copy each. owever, if we simply follow the naive protocol above, we have a problem one node can pretend to be five nodes and collect a  return.  quick fi to this is to encrypt the file five times, using five different keys this makes the five identical copies indistinguishable from five different files, so a storer will not be able to notice that the five files are the same and store them once but claim a  reward.nnut even here we have two problems. irst, there is no way to verify that the five copies of the file are stored by five separate users. f you want to have your file backed up by a decentralized cloud, you are paying for the service of decentralization it makes the protocol have much less utility if all five users are actually storing everything through oogle and mazon. his is actually a hard problem although encrypting the file five times and pretending that you are storing five different files will prevent a single actor from collecting a  reward with  storage, it cannot prevent an actor from collecting a  reward with  storage, and economies of scale mean even that situation will be desirable from the point of view of some storers. econd, there is the issue that you are taking a large overhead, and especially taking the false-redundancy issue into account you are really not getting that much redundancy from it - for eample, if a single node has a % chance of being offline (quite reasonable if we're talking about a network of files being stored in the spare space on people's hard drives), then you have a .% chance at any point that the file will be inaccessible outright.nnhere is one solution to the first problem, although it is imperfect and it's not clear if the benefits are worth it. he idea is to use a combination of proof of stake and a protocol called "nnproof of custodynn" - proof of simultaneous possession of a file and a private key. f you want to store your file, the idea is to randomly select some number of stakeholders in some currency, weighting the probability of selection by the number of coins that they have. mplementing this in an thereum contract might involve having participants deposit ether in the contract (remember, deposits are trust-free here if the contract provides a way to withdraw) and then giving each account a probability proportional to its deposit. hese stakeholders will then receive the opportunity to store the file. hen, instead of the simple erkle tree check described in the previous section, the proof of custody protocol is used.nnhe proof of custody protocol has the benefit that it is non-outsourceable - there is no way to put the file onto a server without giving the server access to your private key at the same time. his means that, at least in theory, users will be much less inclined to store large quantities of files on centralized "cloud" computing systems. f course, the protocol accomplishes this at the cost of much higher verification overhead, so that leaves open the question do we want the verification overhead of proof of custody, or the storage overhead of having etra redundant copies just in casenn of nnegardless of whether proof of custody is a good idea, the net step is to see if we can do a little better with redundancy than the naive replication paradigm. irst, let's analyze how good the naive replication paradigm is. uppose that each node is available % of the time, and you are willing to take  overhead. n those cases, the chance of failure isnn.nn^nnnnnn.nn- a rather high value compared to the "four nines" (ie. .% uptime) offered by centralized services (some centralized services offer five or si nines, but purely because ofnnalebian black swan considerationsnnany promises over three nines can generally be considered bunk because decentralized networks do not depend on the eistence or actions of any specific company or hopefully any specific software package, however, decentralized systems arguably actually can promise something like four nines legitimately). f we assume that the majority of the network will be quasi-professional miners, then we can reduce the unavailability percentage to something like %, in which case we actually do get four nines, but it's better to assume the more pessimistic case.nnhat we thus need is some kind of -of- protocol, much likennmultisig for itcoinnn. o let's describe our dream protocol first, and worry about whether it's feasible later. uppose that we have a file of  , and we want to "multisig" it into a -of- setup. e split the file up into  chunks, each   each (ie.   total), such thatnnanynn of those chunks suffice to reconstruct the original. his is information-theoretically optimal you can't reconstruct a gigabyte out of less than a gigabyte, but reconstructing a gigabyte out of a gigabyte is entirely possible. f we have this kind of protocol, we can use it to split each file up into  pieces, encrypt the  chunks separately to make them look like independent files, and use an incentivized file storage protocol on each one separately.nnow, here comes the fun part such a protocol actually eists. n this net part of the article, we are going to describe a piece of math that is alternately called either "secret sharing" or "erasure coding" depending on its application the algorithm used for both those names is basically the same with the eception of one implementation detail. o start off, we will recall a simple insight two points make a line.nnarticularly, note that there is eactly one line that passes through those two points, and yet there is an infinite number of lines that pass through one point (and an infinite number of lines that pass through zero points). ut of this simple insight, we can make a restricted -of-n version of our encoding treat the first half of the file as the y coordinate of a line atnnnnnnnnand the second half as the y coordinate of the line atnnnnnnnn, draw the line, and take points atnnnnnnnn,nnnnnnnn, etc. ny two pieces can then be used to reconstruct the line, and from there derive the y coordinates atnnnnnnnnandnnnnnnnnto get the file back.nnathematically, there are two ways of doing this. he first is a relatively simple approach involving a system of linear equations. uppose that we file we want to split up is the number "". he left half is , the right half is , so the line joinsnn(, )nnandnn(, )nn. f we want to determine the slope and y-intercept of the line, we can just solve the system of linear equationsnnubtract the first equation from the second, and you getnnnd then plug that into the first equation, and getnno we have our equation,nny   *  + nn. e can now generate new pointsnn(, )nn,nn(, )nn, etc. nd from any two of those points we can recover the original equation.nnow, let's go one step further, and generalize this into m-of-n. s it turns out, it's more complicated but not too difficult. e know that two points make a line. e also know that three points make a parabolannhus, for -of-n, we just split the file into three, take a parabola with those three pieces as the y coordinates atnnnnnnnn,nnnn,nnnn, and take further points on the parabola as additional pieces. f we want -of-n, we use a cubic polynomial instead. et's go through that latter case we still keep our original file, "", but we'll split it up using -of- instead. ur four points arenn(nnnn,nnnn)nn,nn(nnnn,nnnn)nn,nn(nnnn,nnnn)nn,nn(nnnn,nnnn)nn. o we havennek! ell, let's, uh, start subtracting. e'll subtract equation  from equation ,  from , and  from , to reduce four equations to three, and then repeat that process again and again.nnonna  /nn. ow, we unravel the onion, and getnnonnb  -/nn, and thennnonnc  nn, and thennnonna  .nn,nnb  -.nn,nnc  nn,nnd  -nn. ere's the lovely polynomial visualizednn created a ython utility to help you do this (this utility also does other more advanced stuff, but we'll get into that later) you can download itnnherenn. f you wanted to solve the equations quickly, you would just type innnnnimportnnsharennnnshare.sys_solvenn(nnnnnn.nn,nn.nn,nn.nn,nn.nn, -.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn, -.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn, -.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn, -.nn]nn]nn)nnnn.nn, -.,nn.nn, -.nn]nnote that putting the values in as floating point is necessary if you use integers ython's integer division will screw things up.nnow, we'll cover the easier way to do it, agrange interpolation. he idea here is very clever we come up with a cubic polynomial whose value isnnnnatnn  nnandnnnnatnn  , , nn, and do the same for every other  coordinate. hen, we multiply and add the polynomials together for eample, to matchnn(, , , )nnwe simply take  the polynomial that passes throughnn(, , , )nn,  the polynomial throughnn(, , , )nn,  the polynomial throughnn(, , , )nnand  the polynomial throughnn(, , , )nnand then add those polynomials together to get the polynomal throughnn(, , , )nn(note that  saidnnthennpolynomial passing throughnn(, , , )nn the trick works because four points define a cubic polynomial uniquely). his might not seem easier, because the only way we have of fitting polynomials to points to far is the cumbersome procedure above, but fortunately, we actually have an eplicit construction for itnntnn  nn, notice that the top and bottom are identical, so the value is . tnn  , , nn, however, one of the terms on the top is zero, so the value is zero. ultiplying up the polynomials takes quadratic time (ie. ~ steps for  equations), whereas our earlier procedure took cubic time (ie. ~ steps for  equations), so it's a substantial improvement especially once we start talking about larger splits like -of-. he python utility supports this algorithm toonnnnimportnnsharennnnshare.lagrange_interpnn(nnnn.nn,nn.nn,nn.nn,nn.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn]nn)nnnn-.,nn.nn, -.,nn.nn]nnhe first argument is the y coordinates, the second is the  coordinates. ote the opposite order here the code in the python module puts the lower-order coefficients of the polynomial first. nd finally, let's get our additional sharesnnnnshare.eval_poly_atnn(nnnn-.,nn.nn, -.,nn.nn]nn,nnnn)nn.nnnnshare.eval_poly_atnn(nnnn-.,nn.nn, -.,nn.nn]nn,nnnn)nn.nnnnshare.eval_poly_atnn(nnnn-.,nn.nn, -.,nn.nn]nn,nnnn)nn.nno here immediately we can see two problems. irst, it looks like computerized floating point numbers aren't infinitely precise after all the  turned into .. econd, the chunks start getting large as we move further out atnn  nn, it goes up to . his is somewhat breaking the promise that the amount of data you need to recover the file is the same size as the original file if we losenn  , , , nnthen you need  digits to get the original values back and not . hese are both serious issues, and ones that we will resolve with some more mathematical cleverness later, but we'll leave them aside for now.nnven with those issues remaining, we have basically achieved victory, so let's calculate our spoils. f we use a -of- split, and each node is online % of the time, then we can use combinatorics - specifically, thennbinomial distribution formulann- to compute the probability that our data is okay. irst, to set things upnnnndef facnn(nnnnn)nnnnreturnnnnnifnnnnnnnnnelsennn * facnn(nnn-nn)nnnndef choosenn(nnn,knn)nnnnreturnnnfacnn(nnnnn)nn/ facnn(nnknn)nn/ facnn(nnn-knn)nnnndef probnn(nnn,k,pnn)nnnnreturnnnchoosenn(nnn,knn)nn* p ** k *nn(nnnn-pnn)nn**nn(nnn-knn)nnhe last formula computes the probability that eactly k servers out of n will be online if each individual server has a probability p of being online. ow, we'll donnnnsumnn(nnnnprobnn(nnnn, k,nn.nn)nnfornnknninnnrangenn(nnnn,nnnn)nn]nn)nn.nn.% uptime with only  redundancy - a good step up from the .% uptime that  redundancy would have given us had simple replication been the only tool in our toolkit. f we crank the redundancy up to , then we get si nines, and we can stop there because the probability either thereum or the entire internet will crash outright is greater than .% anyway (in fact, you'rennmore likely to dienntomorrow). h, and if we assume each machine has % uptime (ie. hobbyist "farmers"), then with a .-redundant -of- protocol we get an absolutely overkill twelve nines. eputation systems can be used to keep track of how often each node is online.nnealing with rrorsnne'll spend the rest of this article discussing three etensions to this scheme. he first is a concern that you may have skipped over reading the above description, but one which is nonetheless important what happens if some node tries to actively cheat he algorithm above can recover the original data of a -of- split from any  pieces, but what if one of the data providers is evil and tries to provide fake data to screw with the algorithm. he attack vector is a rather compelling onennnnshare.lagrange_interpnn(nnnn.nn,nn.nn,nn.nn,nn.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn]nn)nnnn-.,nn.nn, -.,nn.nn]nnaking the four points of the above polynomial, but changing the last value to , gives a completely different result. here are two ways of dealing with this problem. ne is the obvious way, and the other is the mathematically clever way. he obvious way is obvious when splitting a file, keep the hash of each chunk, and compare the chunk against the hash when receiving it. hunks that do not match their hashes are to be discarded.nnhe clever way is somewhat more clever it involves some spooky not-quite-moon-math called thennerlekamp-elch algorithmnn. he idea is that instead of fitting just one polynomial,nnnn, we imagine into eistence two polynomials,nnnnandnnnn, such thatnn()  () * ()nn, and try to solve for bothnnnnandnnnnat the same time. hen, we computenn   / nn. he idea is that if the equation holds true, then for allnnnneithernn()  () / ()nnornn()  nn hence, aside from computing the original polynomial we magically isolate what the errors are.  won't go into an eample here the ikipedia article has a perfectly decent one, and you can try it yourself withnnnnmapnn(nnlambda  share.eval_poly_atnn(nnnn-.,nn.nn, -.,nn.nn]nn, nn)nn,nnnnnn,nnnn,nnnn,nnnn,nnnn,nnnn]nn)nnnn.nn,nn.nn,nn.nn,nn.nn,nn.nn,nn.nn]nnnnshare.berlekamp_welch_attemptnn(nnnn.nn,nn.nn,nn.nn,nn.nn,nn.nn,nn.nn]nn,nnnnnn,nnnn,nnnn,nnnn,nnnn,nnnn]nn,nnnn)nnnn-.,nn.nn, -.,nn.nn]nnnnshare.berlekamp_welch_attemptnn(nnnn.nn,nn.nn,nn.nn,nn.nn,nn.nn,nn.nn]nn,nnnnnn,nnnn,nnnn,nnnn,nnnn,nnnn]nn,nnnn)nnnn-.,nn.nn, -.,nn.nn]nnow, as  mentioned, this mathematical trickery is not really all that needed for file storage the simpler approach of storing hashes and discarding any piece that does not match the recorded hash works just fine. ut it is incidentally quite useful for another application self-healing itcoin addresses. itcoin has annbasechecknnencoding algorithm, which can be used to detect when a itcoin address has been mistyped and returns an error so you do not accidentally send thousands of dollars into the abyss. owever, using what we know, we can actually do better and make an algorithm which not only detects mistypes but also actually corrects the errors on the fly. e don't use any kind of clever address encoding for thereum because we prefer to encourage use of name registry-based alternatives, but if an address encoding scheme was demanded something like this could be used.nninite ieldsnnow, we get back to the second problem once our  coordinates get a little higher, the y coordinates start shooting off very quickly toward infinity. o solve this, what we are going to do is nothing short of completely redefining the rules of arithmetic as we know them. pecifically, let's redefine our arithmetic operations asnna + b nnnn(nna + bnn)nn%nnnna - b nnnn(nna - bnn)nn%nnnna * b nnnn(nna * bnn)nn%nnnna / b nnnn(nna * b **nnnn)nn%nnnnhat "percent" sign there is "modulo", ie. "take the remainder of dividing that vaue by ", so we havennnn+nnnnnnnn,nnnn*nnnnnnnn(and its corollarynnnn/nnnnnnnn), etc. e are now only allowed to deal with the numbers , , , , , , , , , , . he surprising thing is that, even as we do this, all of the rules about traditional arithmetic still hold with our new arithmeticnn(nna * bnn)nn* cnnnna *nn(nnb * cnn)nn,nn(nna + bnn)nn* cnnnn(nna * cnn)nn+nn(nnb * cnn)nn,nna / b * bnnnnannifnnbnn!nnnn,nn(nna^ - b^nn)nnnn(nna - bnn)nn*nn(nna + bnn)nn, etc. hus, we can simply take the algebra behind our polynomial encoding that we used above, and transplant it over into the new system. ven though the intuition of a polynomial curve is completely borked - we're now dealing with abstract mathematical objects and not anything resembling actual points on a plane - because our new algebra is self-consistent, the formulas still work, and that's what counts.nnnnennnnshare.mkodulolassnn(nnnn)nnnnnnnnshare.lagrange_interpnn(nnmapnn(nne,nnnnnn,nnnn,nnnn,nnnn]nn)nn, mapnn(nne,nnnnnn,nnnn,nnnn,nnnn]nn))nnnnnnnnnn,nnnn,nnnn,nnnn]nnnnmapnn(nnlambda  share.eval_poly_atnn(nnmapnn(nne, nn)nn, enn(nnnn))nn, rangenn(nnnn,nnnn))nnnnnn,nnnn,nnnn,nnnn,nnnn,nnnn,nnnn,nnnn]nnnnshare.berlekamp_welch_attemptnn(nnmapnn(nne,nnnnnn,nnnn,nnnn,nnnn,nnnn,nnnn,nnnn,nnnn]nn)nn, mapnn(nne,nnnnnn,nnnn,nnnn,nnnn,nnnn,nnnn,nnnn,nnnn]nn)nn,nnnn)nnnnnn,nnnn,nnnn,nnnn]nnhe "nnmapnn(nne,nnnnv, v, vnn]nn)nn" is used to convert ordinary integers into elements in this new field the software library includes an implementation of our crazy modulo  numbers that interfaces with arithmetic operators seamlessly so we can simply swap them in (eg.nnprint enn(nnnn)nn* enn(nnnn)nnreturnsnnnn). ou can see that everything still works - ecept that now, because our new definitions of addition, subtraction, multiplication and division always return integers innnnnnn..nn.nnnn]nnwe never need to worry about either floating point imprecision or the numbers epanding as the  coordinate gets too high.nnow, in reality these relatively simple modulo finite fields are not what are usually used in error-correcting codes the generally preferred construction is something called annalois fieldnn(technically, any field with a finite number of elements is a alois field, but sometimes the term is used specifically to refer to polynomial-based fields as we will describe here). he idea is that the elements in the field are now polynomials, where the coefficients are themselves values in the field of integers modulo  (ie.nna + b  (a + b) % nn, etc). dding and subtracting work as normally, but multiplying is itself modulo a polynomial, specificallynn^ + ^ + ^ +  + nn. his rather complicated multilayered construction lets us have a field with eactly  elements, so we can conveniently store every element in one byte and every byte as one element. f we want to work on chunks of many bytes at a time, we simply apply the scheme in parallel (ie. if each chunk is  bytes, determine  polynomials, one for each byte, etend them separately, and combine the values at each  coordinate to get the chunk there).nnut it is not important to know the eact workings of this the salient point is that we can redefinenn+nn,nn-nn,nn*nnandnn/nnin such a way that they are still fully self-consistent but always take and output bytes.nnoing ultidimensional he elf-ealing ubennow, we're using finite fields, and we can deal with errors, but one issue still remains what happens when nodes do go down t any point in time, you can count on % of the nodes storing your file staying online, but what you cannot count on is the same nodes staying online forever - eventually, a few nodes are going to drop out, then a few more, then a few more, until eventually there are not enough of the original nodes left online. ow do we fight this gradual attrition ne strategy is that you could simply watch the contracts that are rewarding each individual file storage instance, seeing when some stop paying out rewards, and then re-upload the file. owever, there is a problem in order to re-upload the file, you need to reconstruct the file in its entirety, a potentially difficult task for the multi-gigabyte movies that are now needed to satisfy people's seemingly insatiable desires for multi-thousand piel resolution. dditionally, ideally we would like the network to be able to heal itself without requiring active involvement from a centralized source, even the owner of the files.nnortunately, such an algorithm eists, and all we need to accomplish it is a clever etension of the error correcting codes that we described above. he fundamental idea that we can rely on is the fact that polynomial error correcting codes are "linear", a mathematical term which basically means that it interoperates nicely with multiplication and addition. or eample, considernnnnshare.lagrange_interpnn(nnnn.nn,nn.nn,nn.nn,nn.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn]nn)nnnn-.,nn.nn, -.,nn.nn]nnnnshare.lagrange_interpnn(nnnn.nn,nn.nn,nn.nn,nn.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn]nn)nnnn.nn, -.,nn.nn,nn.nn]nnnnshare.lagrange_interpnn(nnnn.nn,nn.nn,nn.nn,nn.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn]nn)nnnn.nn, -., -.,nn.nn]nnnnshare.lagrange_interpnn(nnnn.nn,nn.nn,nn.nn,nn.nn]nn,nnnn.nn,nn.nn,nn.nn,nn.nn]nn)nnnn.nn, -., -.,nn.nn]nnee how the input to the third interpolation is the sum of the inputs to the first two, and the output ends up being the sum of the first two outputs, and then when we double the input it also doubles the output. o what's the benefit of this ell, here's the clever trick. rasure cording is itself a linear formula it relies only on multiplication and addition. ence, we are going tonnapply erasure coding to itselfnn. o how are we going to do this ere is one possible strategy.nnirst, we take our -digit "file" and put it into a  grid.nnnnnnnnnnhen, we use the same polynomial interpolation and etension process as above to etend the file along both the  and y aesnnnnnnnnnnnnnnnnnnnnnnnnnnnd then we apply the process again to get the remaining  squaresnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnote that it doesn't matter if we get the last four squares by epanding horizontally and vertically because secret sharing is linear it is commutative with itself, so you get the eact same answer either way. ow, suppose we lose a number in the middle, say, . ell, we can do a repair verticallynnnnshare.repairnn(nnnnnn,nnnn, one,nnnn]nn, enn)nnnnnn,nnnn,nnnn,nnnn]nnr horizontallynnnnshare.repairnn(nnnnnn,nnnn, one,nnnn]nn, enn)nnnnnn,nnnn,nnnn,nnnn]nnnd tada, we get  in both cases. his is the surprising thing the polynomials work equally well on both the  or the y ais. ence, if we take these  pieces from the grid, and split them up among  nodes, and one of the nodes disappears, then nodes along either ais can come together and reconstruct the data that was held by that particular node and start claiming the reward for storing that data. deally, we can even etend this process beyond  dimensions, producing a -dimensional cube, a -dimensional hypercube or more - the gain of using more dimensions is ease of reconstruction, and the cost is a lower degree of redundancy. hus, what we have is an information-theoretic equivalent of something that sounds like it came straight out of science-fiction a highly redundant, interlinking, modular self-healing cube, that can quickly locally detect and fi its own errors even if large sections of the cube were to be damaged, co-opted or destroyed.nn"he cube can still function even if up to % of it were to be destroyed..."nno, let's put it all together. ou have a   file, and you want to split it up across the network. irst, you encrypt the file, and then you split the file into, let's say,  chunks. ou arrange these chunks into a -dimensional  cube, figure out the polynomial along each ais, and "etend" each one so that at the end you have a  cube. ou then look for  nodes willing to store each piece of data, and tell each node only the identity of the other nodes that are along the same ais (we want to make an effort to avoid a single node gathering together an entire line, square or cube and storing it and calculating any redundant chunks as needed in real-time, getting the reward for storing all the chunks of the file without actually providing any redundancy.nnn order to actually retrieve the file, you would send out a request for all of the chunks, then see which of the pieces coming in have the highest bandwidth. ou may use the pay-per-chunk protocol to pay for the sending of the data etortion is not an issue because you have such high redundancy so no one has the monopoly power to deny you the file. s soon as the minimal number of pieces arrive, you would do the math to decrypt the pieces and reconstitute the file locally. erhaps, if the encoding is per-byte, you may even be able to apply this to a outube-like streaming implementation, reconstituting one byte at a time.nnn some sense, there is an unavoidable tradeoff between self-healing and vulnerability to this kind of fake redundancy if parts of the network can come together and recover a missing piece to provide redundancy, then a malicious large actor in the network can recover a missing piece on the fly to provide and charge for fake redundancy. erhaps some scheme involving adding another layer of encryption on each piece, hiding the encryption keys and the addresses of the storers of the individual pieces behind yet another erasure code, and incentivizing the revelation process only at some particular times might form an optimal balance.nnecret haringnnt the beginning of the article,  mentioned another name for the concept of erasure coding, "secret sharing". rom the name, it's easy to see how the two are related if you have an algorithm for splitting data up among  nodes such that  of  nodes are needed to recover it but  of  can't, then another obvious use case is to use the same algorithm for storing private keys - split up your itcoin wallet backup into nine parts, give one to your mother, one to your boss, one to your lawyer, put three into a few safety deposit boes, etc, and if you forget your password then you'll be able to ask each of them individually and chances are at least five will give you your pieces back, but the individuals themselves are sufficiently far apart from each other that they're unlikely to collude with each other. his is a very legitimate thing to do, but there is one implementation detail involved in doing it right.nnhe issue is this even though  of  can't recover the original key,  of  can still come together and have quite a lot of information about it - specifically, four linear equations over five unknowns. his reduces the dimensionality of the choice space by a factor of , so instead of nnnnprivate keys to search through they now have only nnnn. f your key is  bits, that goes down to nnnn- trivial work for a reasonably powerful computer. he way we fi this is by erasure-coding not just the private key, but rather the private key plus  as many bytes of random gook. ore precisely, let the private key be the zero-degree coefficient of the polynomial, pick four random values for the net four coefficients, and take values from that. his makes each piece five times longer, but with the benefit that even  of  now have the entire choice space of nnnnor nnnnto search through.nnonclusionnno there we go, that's an introduction to the power of erasure coding - arguably the single most underhyped set of algorithms (ecept perhaps ) in computer science or cryptography. he ideas here essentially are to file storage what multisig is to smart contracts, allowing you to get the absolutely maimum possible amount of security and redundancy out of whatever ratio of storage overhead you are willing to accept. t's an approach to file storage availability that strictly supersedes the possibilities offered by simple splitting and replication (indeed, replication is actually eactly what you get if you try to apply the algorithm with a -of-n strategy), and can be used to encapsulate and separately handle the problem of redundancy in the same way that encryption encapsulates and separately handles the problem of privacy.nnecentralized file storage is still far from a solved problem although much of the core technology, including erasure coding innnahoe-nn, has already been implemented, there are certainly many minor and not-so-minor implementation details that still need to be solved for such a setup to actually work. n effective reputation system will be required for measuring quality-of-service (eg. a node up % of the time is worth at least  more than a node up % of the time). n some ways, incentivized file storage even depends on effective blockchain scalability having to implicitly pay for the fees of  transactions going to verification contracts every hour is not going to work until transaction fees become far lower than they are today, and until then some more coarse-grained compromises are going to be required. ut then again, pretty much every problem in the cryptocurrency space still has a very long way to go.