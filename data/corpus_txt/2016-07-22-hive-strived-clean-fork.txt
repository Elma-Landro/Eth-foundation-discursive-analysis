Titre: Hive: How we strived for a clean fork\nAuteur: Péter Szilágyi\nDate: July 22, 2016\nURL: https://blog.ethereum.org/2016/07/22/hive-strived-clean-fork\nCatégorie: Non catégorisé\n\n==================================================\n\nhe  soft-fork attempt was difficult. ot only did it turn out that we underestimated the side effects on the consensus protocol (i.e. o vulnerability), but we also managed to introduce a data race into the rushed implementation that was a ticking time bomb. t was not ideal, and even though averted at the last instance, the fast approaching hard-fork deadline looked eerily bleak to say the least. e needed a new strategy...nnhe stepping stone towards this was an idea borrowed from oogle (courtesy of ick ohnson) writing up a detailednnpostmortemnnof the event, aiming to assess the root causes of the issue, focusing solely on the technical aspects and appropriate measures to prevent recurrence.nnechnical solutions scale and persist blaming people does not. ~ icknnrom the postmortem, one interesting discovery from the perspective of this blog post was made. he soft-fork code inside go-ethereum](https//github.com/ethereum/go-ethereum) seemed solid from all perspectives a) it was thoroughly covered by unit tests with a  test-to-code ratio b) it was thoroughly reviewed by si foundation developers and c) it was even manually live tested on a private network... et still, a fatal data race remained, which could have potentially caused severe network disruption.nnt transpired that the flaw could only ever occur in a network consisting of multiple nodes, multiple miners and multiple blocks being minted simultaneously. ven if all of those scenarios held true, there was only a slight chance for the bug to surface. nit tests cannot catch it, code reviewers may or may not catch it, and manual testing catching it would be unlikely. ur conclusion was that the development teams needed more tools to perform reproducible tests that would cover the intricate interplay of multiple nodes in a concurrent networked scenario. ithout such a tool, manually checking the various edge cases is unwieldy and without doing these checks continuously as part of the development workflow, rare errors would become impossible to discover in time.nnnd thus,nnhivennwas born...nnhat is hivennthereum grew large to the point where testing implementations became a huge burden. nit tests are fine for checking various implementation quirks, but validating that a client conforms to some baseline quality, or validating that clients can play nicely together in a multi client environment, is all but simple.nnivennis meant to serve as an easily epandable test harness wherennanyonenncan add tests (be those simple validations or network simulations) innnanynnprogramming language that they are comfortable with, and hive should simultaneously be able to run those tests againstnnallnnpotential clients. s such, the harness is meant to do black bo testing where no client specific internal details/state can be tested and/or inspected, rather emphasis would be put on adherence to official specs or behaviors under different circumstances.nnost importantly, hive was designed from the ground up to run as part of any clients'  workflow!nnow does hive worknnive's body and soul is docker](https//www.docker.com/). very client implementation is a docker image every validation suite is a docker image and every network simulation is a docker image. ive itself is an all encompassing docker image. his is a very powerful abstraction...nnincennthereum clientsnnare docker images in hive, developers of the clients can assemble the best possible environment for their clients to run in (dependency, tooling and configuration wise). ive will spin up as many instances as needed, all of them running in their own inu systems.nnimilarly, asnntest suitesnnvalidating thereum clients are docker images, the writer of the tests can use any programing environment he is most familiar with. ive will ensure a client is running when it starts the tester, which can then validate if the particular client conforms to some desired behavior.nnastly,nnnetwork simulationsnnare yet again defined by docker images, but compared to simple tests, simulators not only eecute code against a running client, but can actually start and terminate clients at will. hese clients run in the same virtual network and can freely (or as dictated by the simulator container) connect to each other, forming an on-demand private thereum network.nnow did hive aid the forknnive is neither a replacement for unit testing nor for thorough reviewing. ll current employed practices are essential to get a clean implementation of any feature. ive can provide validation beyond what's feasible from an average developer's perspective running etensive tests that can require comple eecution environments and checking networking corner cases that can take hours to set up.nnn the case of the  hard-fork, beyond all the consensus and unit tests, we needed to ensure most importantly that nodes partition cleanly into two subsets at the networking level one supporting and one opposing the fork. his was essential since it's impossible to predict what adverse effects running two competing chains in one network might have, especially from the minority's perspective.nns such we've implemented three specific network simulations in hivennhe firstnnto check that miners running the full thash s generate correct block etra-data fields for both pro-forkers and no-forkers, even when trying to naively spoof.nnhe secondnnto verify that a network consisting of mied pro-fork and no-fork nodes/miners correctly splits into two when the fork block arrives, also maintaining the split afterwards.nnhe thirdnnto check that given an already forked network, newly joining nodes can sync, fast sync and light sync to the chain of their choice.nnhe interesting question though is did hive actually catch any errors, or did is just act as an etra confirmation that everything's all right nd the answer is,nnbothnn. ive caughtnnthree fork-unrelated bugsnnin eth, but also heavily aided eth's hard-fork development by continuously providing feedback on how changes affected network behavior.nnhere was some criticism of the go-ethereum team for taking their time on the hard-fork implementation. opefully people will now see what we were up to, while concurrently implementing the fork itself. ll in all,  believennhivennturned out to play quite an important role in the cleanness of this transition.nnhat is hive's futurennhe thereum itub organization features  test tools already](https//github.com/ethereumutf%%%&querytest), with at least one  benchmark tool cooking in some eternal repository. hey are not being utilised to their full etent. hey have a ton of dependencies, generate a ton of junk and are very complicated to use.nnith hive, we're aiming to aggregate all the various scattered tests under onennuniversal client validatornnthat has minimal dependencies, can be etended by anyone, and can run as part of the daily  workflow of client developers.nne welcome anyone to make contributions to the project, be that adding new clients to validate, validators to test with, or simulators to find interesting networking issues. n the meantime, we'll try to further polish hive itself, adding support for running benchmarks as well as mied-client simulations.nnith a bit or work, maybe we'll even have support for running hive in the cloud, allowing it to run network simulations at a much more interesting scale.