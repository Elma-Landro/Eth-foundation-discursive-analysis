Titre: Geth v1.10.0\nAuteur: P√©ter Szil√°gyi\nDate: March 3, 2021\nURL: https://blog.ethereum.org/2021/03/03/geth-v1-10-0\nCat√©gorie: Non cat√©goris√©\n\n==================================================\n\nh wow, it's been a while... over . years since we've releasednneth v..nn. e did do  point releases in that time frame (about one per three weeks), but pushing out a major release is always a bit more special. he adrenaline rush of shipping new features, coupled with the fear of something going horribly wrong. till unsure if  like it or hate it. ither way, thereum is evolving and we need to push the envelope to keep up with it.nnithout further ado, please welcomenneth v..nnto the thereum family.nnere be dragonsnnefore diving into the details of our newest release, it's essential to emphasize thatnnwith any new feature, come new risksnn. o cater for users and projects with differing risk profiles, many of our heavy hitter features can be (for now) toggled on and off individually. hether you read the entire content of this blog post - or only skim parts interesting to you -nnplease read the 'ompatibility' section at the end of this documentnn!nnith that out of the way, let's dive in and see what eth v.. is all about!nnerlin hard-forknnet's get the elephant out of the room first. eth v..nndoes not shipnnthennerlin hard-forknnyet, as there was some th hour concerns from the olidity team about -. ince v.. is a major release, we don't want to publish it too close to the fork. e will follow up with v.. soon with the final list of s and block numbers baked in.nnnapshotsnne've beennntalking about snapshotsnnfor such a long time now, it feels strange to finally see them in a release. ithout going into too many details (see linked post),nnsnapshotsnnare an acceleration data structure on top of the thereum state, that allowsnnreadingnnaccounts and contract storage significantly faster.nno put a number on it, the snapshot feature reduces the cost of accessing an account fromnn(log)nntonn()nn. his might not seem like much at a first glance, but translated to practical terms, on mainnet with  million accounts, snapshots can save about  database lookups per account read. hat's almostnnan order of magnitude lessnndisk lookups, guaranteed constant independent of state size.nnhoa, does this mean we can  the gas limitnno, unfortunately. hilst snapshots do grant us a  read performance,  eecution alsonnwritesnndata, and these writes need to be erkle proven. he erkle proof requirement retains the necessity fornn(log)nndisk access on writes.nno, what's the point then!nnhilst fast read access to accounts and contract storage isn't enough to bump the gas limit, it does solve a few particularly thorny issuesnno.nnn , thereum sustained its worse o attack ever -nnhe hanghai ttacksnn- that lasted about - months. he attack revolved around bloating thereum's state and abusing various underpriced opcodes to grind the network to a halt. fter numerous client optimizations and repricing hard forks, the attack was repelled. he root cause still lingers state access opcodes have a fied  gas costnn()nn, but an ever slowly increasing eecution costnn(log)nn. e've bumped the gas costs innnangerine histlenn,nnstanbulnnand nownnerlinnnto bring the  costs back in line with the runtime costs, but these are stopgap measures. napshots on the other hand reduce eecution cost of state reads tonn()nn- in line with  costs - thus solves the read-based o issues long term (don't quote me on that).nnall.nnhecking a smart contract's state in thereum entails a mini  eecution. art of that is running bytecode and part of it is reading state slots from disk. f you have your personal thereum node that you only use for your own personal needs, there's a high chance that the current state access speed is more than adequate. f you're operating a node for the consumption of multiple users however, the  performance improvement granted by snapshots means that you can serve  as many queries at +- the same cost to you.nnync.nnhere are two major ways you can synchronize an thereum node. ou can download the blocks and eecute all the transactions within or you can download the blocks, verify the os and download the state associated a recent block. he latter is much faster, but it relies on benefactors serving you a copy of the recent state. ith the current erkle-atricia state model, these benefactors readnnnnof data off disk to serve a syncing node. napshots enable serving nodes to read onlynnnnof data off disk to get a new node joined into the network. ore on this in thennnap syncnnsection.nns with all features, it's a game of tradeoffs. hilst snapshots have enormous benefits, that we believe in strongly enough to enable for everyone, there are certain costs to themnn snapshot is a redundant copy of the raw thereum state already contained in the leaves of the erkle atricia trie. s such, snapshots entail an additional disk overhead of aboutnn- on mainnetnncurrently. opefully snapshots will allow us to do some further state optimizations and potentially remove some of the disk overhead of erkle tries as they are currently.nnince nobody has snapshots constructed in the network yet, nodes will initially need to bear the cost of iterating the state trie and creating the initial snapshot themselves. epending on the load to your node, this might take anywhere betweennna day to a weeknn, but you only need to do it once in the lifetime of your node (if things work as intended). he snapshot generation runs in the background, concurrently with all other node operations. e have plans to not require this once snapshots are generally available in the network. ore on this in thennnap syncnnsection.nnf you are not confident about the snapshot feature, younncan disable itnnin eth .. viann--snapshotfalsenn, but be advised that we will make it mandatory long term to guarantee a baseline network health.nnnap syncnnf you thought snapshots took a long time to ship, wait till you hear about snap sync! e've implemented the initial prototype of a new synchronization algorithm way back innnctober, nn... then sat on the idea for over  years! ü§Ø efore diving in, a bit of history.nnhen thereum launched, you could choose from two different ways to synchronize the networknnfull syncnnandnnfast syncnn(omitting light clients from this discussion).nnull syncnnoperated by downloading the entire chain and eecuting all transactions vs.nnfast syncnnplaced an initial trust in a recent-ish block, and directly downloaded the state associated with it (after which it switched to block eecution like full sync). lthough both modes of operation resulted in the same final dataset, they preferred different tradeoffsnnull syncnnminimized trust, choosing to eecute all transactions from genesis to head. hilst it might be the most secure option, thereum mainnet currently contains overnn. billion transactionsnn, growing at a rate ofnn. million / daynn. hosing to eecute everything from genesis meansnnfull syncnnhas a forever increasing cost. urrently it takes - days to process all those transactions on a fairly powerful machine.nnast syncnnchose to rely on the security of the os. nstead of eecuting all transactions, it assumed that a block with  valid os on top would be prohibitively epensive for someone to construct, as such it's ok to download the state associated withnn-nn.nnast syncnntrusting the state root from a recent block, it could download the state trie directly. his replaced the need of  & disk  with a need for network bandwidth and latency. pecifically, thereum mainnet currently contains about  million state trie nodes, taking about - hours to download on a fairly well connected machine.nnull syncnnremained available for anyone who wanted to epend the resources to verify thereum's entire history, but for most people,nnfast syncnnwas more than adequate‚Ñ¢. here's a computer science parado, that once a system reaches  the usage it was designed at, it will break down. he logic is, that irrelevant how something works, push it hard enough and an unforeseen bottleneck will appear.nnn the case ofnnfast syncnn, the unforeseen bottleneck wasnnlatencynn, caused by thereum's data model. thereum's state trie is a erkle tree, where the leaves contain the useful data and each node above is the hash of  children. yncing from the root of the tree (the hash embedded in a block header), the only way to download everything is to request each nodennone-by-onenn. ith  million nodes to download, even by batching  requests together, it ends up needing . million round-trips. ssuming an overly generous ms  to  serving peers,nnfast syncnnis essentiallynnwaiting for over  minutesnnfor data to arrive. ut network latency is only /rd of the problem.nnhen a serving peer receives a request for trie nodes, it needs tonnretrieve them from disknn. thereum's erkle trie doesn't help here either. ince trie nodes are keyed by hash, there's no meaningful way to store/retrieve them batched, each requiring it's own database read. o make matters worse, evel (used by eth) stores data in  levels, so a random read will generally touch as many files. ultiplying it all up, a single network request of  nodes - at  reads a pop - amounts to . thousand disk reads. ith the fastest  s' speed of . , that's ms etra latency. ith the same  serving peer assumption as above,nnfast syncnnjust added annnetra  minutes waiting timenn. ut serving latency is only / of the problem.nnequesting that many trie nodes individually means actually uploading that many hashes to remote peers to serve. ith  million nodes to download, that's  million hashes to upload, or  *  bytes  . t a global average of bps upload speed ( oubt),nnfast syncnnjust added annnetra  minutes waiting timenn. ownloads are a bit more than twice as large, so with global averages of bps, *fast sync* popped on annfurther  minutesnn. andwidth delays are the last / of the problem.nnum it all up, andnnfast syncnnspends a whopping . hours doing nothing, justnnwaiting for datannnnfnnyou have an above average network linknnfnnyou have a good number of serving peersnnfnnyour peers don't serve anyone else but younnnap syncnnwas designed to solve all three of the enumerated problems. he core idea is fairly simple instead of downloading the trie node-by-node,nnsnap syncnndownloads the contiguous chunks of useful state data, and reconstructs the erkle trie locallynnithout downloading intermediate erkle trie nodes, state data can be fetched in large batches, removing the delay caused by network latency.nnithout downloading erkle nodes, downstream data drops to half and without addressing each piece of data individually, upstream data gets insignificant, removing the delay caused by bandwidth.nnithout requesting randomly keyed data, peers do only a couple contiguous disk reads to serve the responses, removing the delay of disk  (nniff the peers already have the data stored in an appropriate flat formatnn).nnhilstnnsnap syncnnis eerily similar to arity'snnwarp syncnn- and indeed took many design ideas from it - there are significant improvements over the latternnarp syncnnrelies onnnstatic snapshotsnncreated every  blocks. his means serving nodes need to regenerate the snapshots every  days or so, but iterating the entire state trie can actually take more time than that. his meansnnwarp syncnnis not sustainable long term. pposed to this,nnsnap syncnnis based onnndynamic snapshotsnn, which are generated only once, no matter how slowly, and then are kept up to date as the chain progresses.nnarp syncnn's snapshot format does not follow the erkle trie layout, and as such chunks of warp-datanncannotnnbe individually proven. yncing nodes need to download the entire + dataset before they can verify it. his meansnnwarp syncingnnnodes could be theoretically grieved. pposed to this,nnsnap syncnn's snapshot format is just the sequential erkle leaves, which allows any range to be proven, thus bad data is detected immediately.nno put a number onnnsnap syncnnvsnnfast syncnn, synchronizing the mainnet state (ignoring blocks and receipts, as those are the same) against  serving peers, at block ~#,, produced the following resultsnno note, thatnnsnap syncnnis shipped, but not yet enabled, in eth v... he reason is that servingnnsnap syncnnrequires nodes to have thennsnapshotnnacceleration structure already generated, which nobody has yet, as it is also shipped in v... ou can manually enable snap sync viann--syncmode snapnn, but be advised that we epect itnnnot to findnnsuitable peers until a few weeks after erlin. e'll enable it by default when we feel there are enough peers to rely on it.nnffline pruningnne're really proud of what we've achieved with eth over the past years. et, there's alwaysnnthat one topicnn, which makes you flinch when asked about. or eth, that topic isnnstate pruningnn. ut what is pruning and why is it needednnhen processing a new block, a node takes the current state of the network as input data and mutates it according to the transactions in the block, generating a new, output data. he output state is mostly the same as the input, only a few thousand items modified. ince we can't just overwrite the old state (otherwise we couldn't handle block reorgs), both old and new end up on disk.nn(k, we're a bit smarter and only push new diffs to disk if they stick around and don't get deleted in the net few blocks, but let's ignore that part for now).nnushing these new pieces of state data, block-by-block, to the database is a problem. hey keep accumulating. n theory we could "just delete" state data that's old enough to not run the risk of a reorg, but as it turns out, that's quite a hard problem. ince state in thereum is stored in a tree data structure - and since most blocks only change a small fraction of the state - these trees share huge portions of the data with one another. e can easily decide if the root of an old trie is stale and can be deleted, but it's eceedingly costly to figure out if a node deep within an old state is still referenced by anything newer or not.nnhroughout the years, we've implemented a range of pruning algorithms to delete leftovers (lost count, around ), yet we've never found a solution that doesn't break down if enough data is thrown at it. s such, people grew accustomed that eth's database starts slim after annfast syncnn, and keeps growing until you get fed up and resync. his is frustrating to say the least, as re-downloading everything just wastes bandwidth and adds meaningless downtime to the node.nneth v.. doesn't quite solve the problem, but it takes a big step towards a better user eperience. f you havennsnapshotsnnenabled and fully generated, eth can use those as an acceleration structure to relatively quickly determine which trie nodes should be kept and which should be deleted. runing trie nodes based on snapshots does have the drawback that the chain may not progress during pruning. his means, that you need to stop eth, prune its database and then restart it.nnecution time wise, pruning takes a few hours (greatly depends on your disk speed and accumulated junk), one third of which is indeing recent trie node from snapshots, one third deleting stale trie nodes and the last third compacting the database to reclaim freed up space. t the end of the process, your disk usage should approimately be the same as if you did a fresh sync. o prune your database, please runnngeth snapshot prune-statenn.nne advised, that pruning is annnew and dangerous featurenn, a failure of which can cause bad blocks. e're confident that it's reliable, but if something goes wrong, there's likely no way to salvage the database. ur recommendation - at least until the feature gets battle tested - is to back up your database prior to pruning, and try with testnet nodes first before going all in on mainnet.nnransaction unindeingnnthereum has been around for a while now, and in its almost  years' of eistence, thereum's users issued overnn billionnntransactions. hat's a big number.nnode operators always took it for granted that they can look up an arbitrary transaction from the past, given only itsnnhashnn. ruth be told, it seems like a no-brainer thing to do. unning the numbers though, we end up in a surprising place. o make transactions searchable, we need to - at minimum - map the entire range of transaction hashes to the blocks they are in. ith all tradeoffs made towards minimizing storage, we still need to store  block number ( bytes) associated with  hash ( bytes).nn bytes / transaction doesn't seem much, but multiplying withnn billionnntransactions ends up at an impressivenn of storagenn, needed to be able to say transactionnndeadbeefnnis in blocknnnn. t's a lot of data and a lot of database entries to shuffle around. toring  is an acceptable price if you want to look up transactions  years back, but in practice, most users don't want to. or them, the etra disk usage and  overhead is wasted resources. t's also important to note that transaction indices are not part of consensus and are not part of the network protocol. hey are purely a locally generated acceleration structure.nnan we shave some - for us - useless data off of our nodes es! eth v.. switches on transaction unindeing by default and sets it tonn,,nnblocks (about  year). he transaction unindeer will linger in the background, and every time a new block arrives, it ensures that only transactions from the most recentnnnnblocks are indeed, deleting older ones. f a user decides they want access to older transactions, they can restart eth with a highernn--tlookuplimitnnvalue, and any blocks missing from the updated range will be reindeed (note, the trigger is still block import, you have to wait for  new block).nnince about /rd ofnnthereum's transaction loadnnhappened in , keeping an entire year's worth of transaction inde will still have a noticeable weight on the database. he goal of transaction unindeing is not to remove an eisting feature in the name of saving space. he goal is to move towards a mode of operation where space does not grow indefinitely with chain history.nnf you wish tonndisablenntransaction unindeing altogether, you can run eth withnn--tlookuplimitnn, which reverts to the old behavior of retaining the lookup map for every transaction since genesis.nnreimage discardingnnthereum stores all its data in a erkle atricia trie. he values in the leaves are the raw data being stored (e.g. storage slot content, account content), and the path to the leaf is the key at which the data is stored. he keys however arennnotnnthe account addresses or storage addresses, rather thenneccaknnhashes of those. his helps balance the branch depths of the state tries. sing hashes for keys is fine as users of thereum only ever reference the original addresses, which can be hashed on the fly.nnhere is one use case, however, where someone has a hash stored in the state trie and wants to recover it's preimage debugging. hen stepping over an  bytecode, a developer might want to glipmse over all the variables in the smart contract. he data is there, but without the preimages, its hard to say which data corresponds to which olidity variable.nnriginally eth had a half-baked solution. e stored in the database all preimages that originated from user calls (e.g. sending a transaction), but not those originating from  calls (e.g. accessing a slot). his was not enough for emi, so we etended our tracing  calls to support saving the preimages for all  (eccak) operations. lthough this solved the debugging issue for emi, it raised the question about all that data unused by non-debugging nodes.nnhe preimages aren't particularly heavy. f you do a full sync from genesis - reeecuting all the transactions - you'll only end up with  etra load. till, there is no reason to keep that data around for users not using it, as it only increases the load on evel compactions. s such, eth v..nndisablesnnpreimage collection by default, but there's no mechanism to actively delete already stored preimages.nnf you are using your eth instance to debug transactions, you cannnretainnnthe original behavior viann--cache.preimagesnn. lease note,nnit is not possible to regenerate preimages after the factnn. f you run eth with preimage collection disabled and change your mind, you'll need to reimport the blocks.nn/ protocolnnhenneth/nnprotocol is a fairly small change, yet has quite a number of beneficial implications. n short, the protocol introduces request and reply s for all bidirectional packets. he goal behind these s is to more easily match up responses to requests, specifically, to more easily deliver a response to a subsystem that made the original request.nnhese s are not essential, and indeed we've been happily working around the lack of them these past  years. nfortunately, all code that needs to request anything from the network becomes overly complicated, if multiple subsystems can request the same type of data concurrently. .g. block headers can be requested by thenndownloadernnsyncing the chain it can be requested by thennfetchernnfulfilling block announcements and it can be requested by fork challenges. urthermore, timeouts can cause late/unepected deliveries or re-requests. n all these cases, when annheader packetnnarrives, every subsystem peeks at the data and tries to figure out if it was meant for itself or someone else. onsuming a reply not meant for a particular subsystem will cause a failure elsewhere, which needs graceful handling. t just gets messy. oable, but messy.nnhe importance ofnneth/nnin the scope of this blog post is not that it solves a particular problem, rather thatnnit is introduced prior to the erlin hard-forknn. s all nodes are epected to upgrade by the fork time, this means eth can start deprecating the old protocols after the fork. nly after discontinuing all older protocols can we rewrite eth's internals to take advantage of request ids. ollowing ournnprotocol deprecation schedulenn, we'll be droppingnneth/nnshortly andnnethnnby the end of summer.nnome people might consider eth using its weight to force protocol updates on other clients. e'd like to emphasize that thenntyped transactionsnnfeature from the erlin hard-fork originally called for a new protocol version. s only eth implemented the full suite ofnneth/ynnprotocols, other clients requested "hacking" it into old protocol versions to avoid having to focus on networking at this time. he agreement was that eth backports typed transaction support into all its old protocol code to buy other devs time, but in echange will phase out the old versions in  months to avoid stagnation.nnhain enforcementnnay back in , whennnhe hard-forknnpassed, thereum introduced the notion of thennchain idnn. he goal was to modify the digital signatures on transactions with a unique identifier to differentiate between what's valid on thereum and what's valid on thereum lassic (and what's valid on testnets). aking a transaction valid on one network but invalid on another ensures they cannot be replayed without the owner's knowledge.nnn order to minimize issues around the transition, both new/protected and old/unprotected transactions remained valid. ast forward  years, and about % of transaction on thereum are still not replay-protected. his doesn't mean there's an inherent vulnerability, unless you reuse the same keys across multiple networks.nnop tip on't!nntill, accidents happen, and certain thereum based networks have been known to go offline due to replay issues.nns much as we don't want to play big brother, we've decided to try and nudge people and tooling to abandon the old, unprotected signatures and use chain ids everywhere. he easy way would be to just make unprotected transactions invalid at the consensus level, but that would leave % of people stranded and scattering for hotfies. o gradually move people towards safer alternatives without pulling the rug from underneath their feet, eth v.. willnnreject transactionsnnon the  that are not replay protected.nnropagation through the  protocols remains unchangednnfor now, but we will be pushing for rejection there too long term.nnf you are using code generated bynnabigennn,nnwe've includednnin thenngo-ethereumnnlibraries additional signer constructors to allow easily creating chain-id-bound transactors. he legacy signers included out of the bo were written before  and until now you needed to construct the protected signer yourself. s this was error prone and some people assumed we guessed the chain  internally, we decided to introduce direct s ourselves.nne will deprecate and remove the legacy signers in the long termnn.nnince we realize people/tooling issuing unprotected transactions can't change overnight, eth v..nnsupports revertingnnto the old behavior and accepting non- transactions viann--rpc.allow-unprotected-tsnn. e advised that this is a temporary mechanism thatnnwill be removednnlong term.nnatabase introspectionnnvery now and again we receive an issue report about a corrupted database, with no real way to debug it. hipping a  data directory to us is not feasible, and sending custom dissection tools to users is cumbersome. lso since a corrupted database often manifests itself in an inability to start up eth, even using debugging  s are useless.nneth v.. ships a built-in database introspection tool to try and alleviate the situation a bit. t is a very low level accessor to evel, but it allows arbitrary data retrievals, insertions and deletions. e are unsure how useful these will turn out to be, but they at least give a fighting chance to restore a broken node without having to resync.nnhe supported commands arenngeth db inspectnn- nspect the storage size for each type of data in the databasenngeth db statsnn- rint various database usage and compaction statisticsnngeth db compactnn- ompact the database, optimizing read access (nnsuper slownn)nngeth db getnn- etrieve and print the value of a database keynngeth db deletenn- elete a database key (nnsuper dangerousnn)nngeth db putnn- et the value of a database key (nnsuper dangerousnn)nnlag deprecationsnnhroughout the v.. release family we've marked a number of  flags deprecated. ome of them were renamed to better follow our naming conventions, others were removed due to dropped features (notably hisper). hroughout the previous release family, we've kept the old deprecated flags functional too, only printing a warning when used instead of the recommended versions.nneth v.. takes the opportunity to completely remove support for the old  flags. elow is a list to help you fi your commands if you by any chance haven't yet upgraded to the new versions the past yearnn--rpcnn-nn--httpnn- nable the - servernn--rpcaddrnn-nn--http.addrnn- - server listening interfacenn--rpcportnn-nn--http.portnn- - server listening portnn--rpccorsdomainnn-nn--http.corsdomainnn- omain from which to accept requestsnn--rpcvhostsnn-nn--http.vhostsnn- irtual hostnames from which to accept requestsnn--rpcapinn-nn--http.apinn- 's offered over the - interfacenn--wsaddrnn-nn--ws.addrnn- - server listening interfacenn--wsportnn-nn--ws.portnn- - server listening portnn--wsoriginsnn-nn--ws.originsnn- rigins from which to accept websockets requestsnn--wsapinn-nn--ws.apinn- 's offered over the - interfacenn--gpoblocksnn-nn--gpo.blocksnn- umber of blocks to check for gas pricesnn--gpopercentilenn-nn--gpo.percentilenn- ercentile of recent ts to use as gas suggestionnn--graphql.addrnn-nn--graphqlnn- nable raph on the - servernn--graphql.portnn-nn--graphqlnn- nable raph on the - servernn--pprofportnn-nn--pprof.portnn- rofiler  server listening portnn--pprofaddrnn-nn--pprof.addrnn- rofiler  server listening interfacenn--memprofileratenn-nn--pprof.memprofileratenn- urn on memory profiling with the given ratenn--blockprofileratenn-nn--pprof.blockprofileratenn- urn on block profiling with the given ratenn--cpuprofilenn-nn--pprof.cpuprofilenn- rite  profile to the given filenn handful of the above listed legacy flags may still work for a few releases, but you should not rely on them remaining available.nnince most people running full nodes do not use  wallets through eth - and since  handling is a bit quirky on different platforms - a lot of node operators just had to eplicitly turn off  viann--nosubnn. o cater the defaults to the requirements of the many, eth v.. disabled  wallet support by default and deprecated thenn--nousbnnflag.nnou can still use  wallets, just need to eplicitly request it from now on viann--usbnn.nnnclean shutdown trackingnnairly often we receive bug reports that eth started importing old blocks on startup. his phenomenon is generally caused by the node operator terminating eth abruptly (power outage,  killer, too short shutdown timeout). ince eth keeps a lot of dirty state in memory - to avoid writing to disk things that get stale a few blocks later - an abrupt shutdown can cause these to not be flushed. ith recent state missing on startup, eth has no choice but to rewind it's local chain to the point where it last saved the progress.nno avoid debating whether an operator did or did not shut down their node cleanly, and to avoid having a clean cycle after a crash hide the fact that data was lost, eth v.. will startnntracking and reportingnnnode crashes. e're hopeful that this will allow operatos to detect that their infra is misconfigured or has issue before those turn into irreversible data loss.nnnnnn-nn|nn.nn]nnncleannnshutdownnndetectednnbootednnnnnn--+nnagennnnwdhnnompatibilitynnoing a major release so close to a hard fork isnnless than desirednn, to say the least. nfortunately, shipping all the large features for the net generation eth took  months longer than we've anticipated. o try and mitigate production problems that might occur from the upgrade,nnalmost all new featuresnncan be toggled off via  flags. here is still  weeks left until the currently planned mainnet block, to ensure you have a smooth eperience. onetheless,nnwe apologize for any inconveniences in advancenn.nno revert as much functionality as possible to the v.. feature-set, please run eth withnn--snapshotfalsennto disable the snapshot acceleration structure and snap syncnn--tlookuplimitnnto keep indeing all transactions, not just the last yearnn--cache.preimagesnntp keep generating and persisting account preimagesnn--rpc.allow-unprotected-tsnn- to allow non-replay-protected signaturesnn--usbnn- to reenable the  wallet supportnnote, thenneth_protocolersionnn call is gone as it made no sense. f you have annvery good reasonnnas to why it's needed, please reach out to discuss it.nnpiloguenns with previous major releases, we‚Äôre really proud of this one too. e've delayed it quite a lot, but we did it in the name of stability to ensure that all the sensitive features are tested as well as we could. e're hopeful this new release family will open the doors to a bit more transaction throughput and a bit lower fees.nns with all our previous releases, you can find thennource code, git tags and whatnot on ournnitub release pagenn.nnre-built binaries for all platforms on ournndownloads pagenn.nnocker images published undernnethereum/client-gonn.nnbuntu packages in ournnaunchpad  repositorynn.nn packages in ournnomebrew ap repositorynn.