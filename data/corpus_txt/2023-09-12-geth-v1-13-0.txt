Titre: Geth v1.13.0\nAuteur: PÃ©ter SzilÃ¡gyi\nDate: September 12, 2023\nURL: https://blog.ethereum.org/2023/09/12/geth-v1-13-0\nCatÃ©gorie: Non catÃ©gorisÃ©\n\n==================================================\n\neth v. comes fairly close on the heels of the . release family, which is funky, considering it's main feature has been in development for a cool  years now. ðŸ¤¯nnhis post will go into a number of technical and historical details, but if you just want the gist of it, eth v.. ships a new database model for storing the thereum state, which is both faster than the previous scheme, and also has proper pruning implemented. o more junk accumulating on disk and no more guerilla (offline) pruning!nnÂ¹cluding ~ ancient data, the same across all configurations.nnÂ²ash scheme full sync eceeded our .  at block ~..nnÂ³ize difference vs snap sync attributed to compaction overhead.nnefore going ahead though, a shoutout goes to ary ong who has been working on the cru of this rework for the better part of  years now! mazing work and amazing endurance to get this huge chunk of work in!nnory tech detailsnnk, so what's up with this new data model and why was it needed in the first placennn short, our old way of storing the thereum state did not allow us to efficiently prune it. e had a variety of hacks and tricks to accumulate junk slower in the database, but we nonetheless kept accumulating it indefinitely. sers could stop their node and prune it offline or resync the state to get rid of the junk. ut it was a very non-ideal solution.nnn order to implement and shipnnrealnnpruning one that does not leave any junk behind, we needed to break a lot of eggs within eth's codebase. ffort wise, we'd compare it to the erge, only restricted to eth's internal levelnntoring state trie nodes by hashes introduces an implicit deduplication (i.e. if two branches of the trie share the same content (more probable for contract storages), they get stored only once). his implicit deduplication means that we can never know how many parent's (i.e. different trie paths, different contracts) reference some node and as such, we can never know what is safe and what is unsafe to delete from disk.nnny form of deduplication across different paths in the trie had to go before pruning could be implemented. ur new data model stores state trie nodes keyed by their path, not their hash. his slight change means that if previously two branches has the same hash and were stored only once now they will have different paths leading to them, so even though they have the same content, they will be stored separately, twice.nntoring multiple state tries in the database introduces a different form of deduplication. or our old data model, where we stored trie nodes keyed by hash, the vast majority of trie nodes stay the same between consecutive blocks. his results in the same issue, that we have no idea how many blocks reference the same state, preventing a pruner from operating effectively. hanging the data model to path based keys makes storing multiple tries impossible altogether the same path-key (e.g. empty path for the root node) will need to store different things for each block.nnhe second invariant we needed to break was the capability to store arbitrarily many states on disk. he only way to have effective pruning, as well as the only way to represent trie nodes keyed by path, was to restrict the database to contain eactly  state trie at any point in time. riginally this trie is the genesis state, after which it needs to follow the chain state as the head is progressing.nnhe simplest solution with storing  state trie on disk is to make it that of the head block. nfortunately, that is overly simplistic and introduces two issues. utating the trie on disk block-by-block entails annlotnnof writes. hilst in sync it may not be that noticeable, but importing many blocks (e.g. full sync or catchup) it becomes unwieldy. he second issue is that before finality, the chain head might wiggle a bit across mini-reorgs. hey are not common, but since theynncannnhappen, eth needs to handle them gracefully. aving the persistent state locked to the head makes it very hard to switch to a different side-chain.nnhe solution is analogous to how eth's snapshots work. he persistent state does not track the chain head, rather it is a number of blocks behind. eth will always maintain the trie changes done in the last  blocks in memory. f there are multiple competing branches, all of them are tracked in memory in a tree shape. s the chain moves forward, the oldets (-) diff layer is flattened down. his permits eth to do blazing fast reorgs within the top  blocks, side-chain switches essentially being free.nnhe diff layers however do not solve the issue that the persistent state needs to move forward on every block (it would just be delayed). o avoid disk writes block-by-block, eth also has a dirty cache in between the persistent state and the diff layers, which accumulates writes. he advantage is that since consecutive blocks tend to change the same storage slots a lot, and the top of the trie is overwritten all the time the dirty buffer short circuits these writes, which will never need to hit disk. hen the buffer gets full however, everything is flushed to disk.nnith the diff layers in place, eth can do  block-deep reorgs instantly. ometimes however, it can be desirable to do a deeper reorg. erhaps the beacon chain is not finalizing or perhaps there was a consensus bug in eth and an upgrade needs to "undo" a larger portion of the chain. reviously eth could just roll back to an old state it had on disk and reprocess blocks on top. ith the new model of having only ever  state on disk, there's nothing to roll back to.nnur solution to this issue is the introduction of a notion called reverse diffs. very time a new block is imported, a diff is created which can be used to convert the post-state of the block back to it's pre-state. he last  of these reverse diffs are stored on disk. henever a very deep reorg is requested, eth can take the persistent state on disk and start applying diffs on top until the state is mutated back to some very old version. hen is can switch to a different side-chain and process blocks on top of that.nnhe above is a condensed summary of what we needed to modify in eth's internals to introduce our new pruner. s you can see, many invariants changed, so much so, that eth essentially operates in a completely different way compared to how the old eth worked. here is no way to simply switch from one model to the other.nne of course recognize that we can't just "stop working" because eth has a new data model, so eth v.. has two modes of operation (talk about  maintanance burden). eth will keep supporting the old data model (furthermore it will stay the default for now), so your node will not do anything "funny" just because you updated eth. ou can even force eth to stick to the old mode of operation longer term viann--state.schemehashnn.nnf you wish to switch to our new mode of operation however, you will need to resync the state (you can keep the ancients ). ou can do it manually or vianngeth removedbnn(when asked, delete the state database, but keep the ancient database). fterwards, start eth withnn--state.schemepathnn. or now, the path-model is not the default one, but if a previous database already eist, and no state scheme is eplicitly requested on the , eth will use whatever is inside the database. ur suggestion is to always specifynn--state.schemepathnnjust to be on the safe side. f no serious issues are surfaced in our path scheme implementation, eth v.. will probably switch over to it as the default format.nn couple notes to keep in mindnnf you are running private eth networks usingnngeth initnn, you will need to specifynn--state.schemennfor the init step too, otherwise you will end up with an old style database.nnor archive node operators, the new data modelnnwillnnbe compatible with archive nodes (and will bring the same amazing database sizes as rigon or eth), but needs a bit more work before it can be enabled.nnlso, a word of warning eth's new path-based storage is considered stable and production ready, but was obviously not battle tested yet outside of the team. veryone is welcome to use it, but if you have significant risks if your node crashes or goes out of consensus, you might want to wait a bit to see if anyone with a lower risk profile hits any issues.nnow onto some side-effect surprises...nnemi-instant shutdownsnnead state missing, repairing chain... ðŸ˜±nn...the startup log message we're all dreading, knowing our node will be offline for hours... is going away!!! ut before saying goodbye to it, lets quickly recap what it was, why it happened, and why it's becoming irrelevant.nnrior to eth v.., the erkle atricia trie of the thereum state was stored on disk as a hash-to-node mapping. eaning, each node in the trie was hashed, and the value of the node (whether leaf or internal node) was inserted in a key-value store, keyed by the computed hash. his was both very elegant from a mathematical perspective, and had a cute optimization that if different parts of the state had the same subtrie, those would get deduplicated on disk. ute... and fatal.nnhen thereum launched, there was only archive mode. very state trie of every block was persisted to disk. imple and elegant. f course, it soon became clear that the storage requirement of having all the historical state saved forever is prohibitive. ast sync did help. y periodically resyncing, you could get a node with only the latest state persisted and then pile only subsequent tries on top. till, the growth rate required more frequent resyncs than tolerable in production.nnhat we needed, was a way to prune historical state that is not relevant anymore for operating a full node. here were a number of proposals, even - implementations in eth, but each had such a huge overhead, that we've discarded them.nneth ended up having a very comple ref-counting in-memory pruner. nstead of writing new states to disk immediately, we kept them in memory. s the blocks progressed, we piled new trie nodes on top and deleted old ones that weren't referenced by the last  blocks. s this memory area got full, we dripped the oldest, still-referenced nodes to disk. hilst far from perfect, this solution was an enormous gain disk growth got drastically cut, and the more memory given, the better the pruning performance.nnhe in-memory pruner however had a caveat it only ever persisted very old, still live nodes keeping anything remotely recent in . hen the user wanted to shut eth down, the recent tries - all kept in memory - needed to be flushed to disk. ut due to the data layout of the state (hash-to-node mapping), inserting hundreds of thousands of trie nodes into the database took many many minutes (random insertion order due to hash keying). f eth was killed faster by the user or a service monitor (systemd, docker, etc), the state stored in memory was lost.nnn the net startup, eth would detect that the state associated with the latest block never got persisted. he only resolution is to start rewinding the chain, until a block is found with the entire state available. ince the pruner only ever drips nodes to disk, this rewind would usually undo everything until the last successful shutdown. eth did occasionally flush an entire dirty trie to disk to dampen this rewind, but that still required hours of processing after a crash.nne dug ourselves a very deep holennhe pruner needed as much memory as it could to be effective. ut the more memory it had, the higher probability of a timeout on shutdown, resulting in data loss and chain rewind. iving it less memory causes more junk to end up on disk.nntate was stored on disk keyed by hash, so it implicitly deduplicated trie nodes. ut deduplication makes it impossible to prune from disk, being prohibitively epensive to ensure nothing references a node anymore across all tries.nneduplicating trie nodes could be done by using a different database layout. ut changing the database layout would have made fast sync inoperable, as the protocol was designed specifically to be served by this data model.nnast sync could be replaced by a different sync algorithm that does not rely on the hash mapping. ut dropping fast sync in favor of another algorithm requires all clients to implement it first, otherwise the network splinters.nn new sync algorithm, one based on state snapshots, instead of tries is very effective, but it requires someone maintaining and serving the snapshots. t is essentially a second consensus critical version of the state.nnt took us quite a while to get out of the above hole (yes, these were the laid out steps all along)nn nap sync's initial designs are made, the necessary supporting data structures are devised.nn eth starts generating and maintaining thennsnapshotnnacceleration structures.nn eth prototypesnnsnap syncnnand defines the final protocol specification.nn eth shipsnnsnap syncnnand switches over to it fromnnfast syncnn.nn ther clients implement consumingnnsnap syncnn.nn eth switches from hash to path keying.nneth becomes incapable of serving the oldnnfast syncnn.nneth reduplicates persisted trie nodes to permit disk pruning.nneth drops in-memory pruning in favor of proper persistent disk pruning.nnne request to other clients at this point is to please implement serving snap sync, not just consuming it. urrently eth is the only participant of the network that maintains the snapshot acceleration structure that all other clients use to sync.nnhere does this very long detour land us ith eth's very core data representation swapped out from hash-keys to path-keys, we could finally drop our beloved in-memory pruner in echange for a shiny new, on-disk pruner, which always keeps the state on disk fresh/recent. f course, our new pruner also uses an in-memory component to make it a bit more optimal, but it primarilly operates on disk, and it's effectiveness is %, independent of how much memory it has to operate in.nnith the new disk data model and reimplemented pruning mechanism, the data kept in memory is small enough to be flushed to disk in a few seconds on shutdown. ut even so, in case of a crash or user/process-manager insta-kill, eth will only ever need to rewind and reeecute a couple hundred blocks to catch up with its prior state.nnay goodbye to the long startup times, eth v.. opens brave new world (withnn--state.schemepathnn, mind you).nnrop thenn--cachennflagnno, we didn't drop thenn--cachennflag, but chances are, you should!nneth'snn--cachennflag has a bit of a murky past, going from a simple (and ineffective) parameter to a very comple beast, where it's behavior is fairly hard to convey and also to properly account.nnack in the rontier days, eth didn't have many parameters to tweak to try and make it go faster. he only optimization we had was a memory allowance for evel to keep more of the recently touched data in . nterestingly, allocating  to evel vs. letting the  cache disk pages in  is not that different. he only time when eplicitly assigning memory to the database is beneficial, is if you have multiple  processes shuffling lots of data, thrashing each other's  caches.nnack then, letting users allocate memory for the database seemed like a good shoot-in-the-dark attempt to make things go a bit faster. urned out it was also a good shoot-yourself-in-the-foot mechanism, as it turned out o's garbage collector really really dislikes large idle memory chunks the  runs when it piles upnnas muchnnjunk, as it had useful data left after the previous run (i.e. it will double the  requirement). hus began the saga ofnnillednnand  crashes...nnast-forward half a decade and thenn--cachennflag, for better or worse, evolvednnepending whether you're on mainnet or testnet,nn--cachenndefaults to  or .nn% of the cache allowance is allocated to the database to use as dumb disk cache.nn% of the cache allowance is allocated to in-memory pruning, % for archive nodes.nn% of the cache allowance is allocated to snapshot caching, % for archive nodes.nn% of the cache allowance is allocated to trie node caching, % for archive nodes.nnhe overall size and each percentage could be individually configured via flags, but let's be honest, nobody understands how to do that or what the effect will be. ost users bumped thenn--cachennup because it lead to less junk accumulating over time (that % part), but it also lead to potential  issues.nnver the past two years we've been working on a variety of changes, to soften the insanitynneth's default database was switched to ebble, which uses caching layers outide of the o runtime.nneth's snapshot and trie node cache started usingnnfastcachenn, also allocating outside of the o runtime.nnhe new path schema prunes state on the fly, so the old pruning allowance was reassigned to the trie cache.nnhe net effect of all these changes are, that using eth's new path database scheme should result in % of the cache being allocated outside of o's  arena. s such, users raising or lowering it should not have any adverse effects on how the  works or how much memory is used by the rest of eth.nnhat said, thenn--cachennflag also has no influece whatsoever any more on pruning or database size, so users who previously tweaked it for this purpose, can drop the flag. sers who just set it high because they had the available  should also consider dropping the flag and seeing how eth behaves without it. he  will still use any free memory for disk caching, so leaving it unset (i.e. lower) will possibly result in a more robust system.nnpiloguenns with all our previous releases, you can find thennource code, git tags and whatnot on ournnitub release pagenn.nnre-built binaries for all platforms on ournndownloads pagenn.nnocker images published undernnethereum/client-gonn.nnbuntu packages in ournnaunchpad  repositorynn.nn packages in ournnomebrew ap repositorynn.