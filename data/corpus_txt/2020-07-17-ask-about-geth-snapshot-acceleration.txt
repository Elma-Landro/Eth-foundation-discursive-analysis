Titre: Ask about Geth: Snapshot acceleration\nAuteur: Péter Szilágyi\nDate: July 17, 2020\nURL: https://blog.ethereum.org/2020/07/17/ask-about-geth-snapshot-acceleration\nCatégorie: Non catégorisé\n\n==================================================\n\n*his is part # of annseriesnnwhere anyone can ask questions about eth and 'll attempt to answer the highest voted one each week with a mini writeup. his week's highest voted question wasnnould you share how the flat db structure is different from the legacy structure*nntate in thereumnnefore diving into an acceleration structure, let's recap a bit what thereum callsnnstatennand how it is stored currently at its various levels of abstraction.nnthereum maintains two different types of state the set of accounts and a set of storage slots for each contract account. rom annpurely abstract perspectivenn, both of these are simple key/value mappings. he set of accounts maps addresses to their nonce, balance, etc.  storage area of a single contract maps arbitrary keys - defined and used by the contract - to arbitrary values.nnnfortunately, whilst storing these key-value pairs as flat data would be very efficient, verifying their correctness becomes computationally intractable. very time a modification would be made, we'd need to hash all that data from scratch.nnnstead of hashing the entire dataset all the time, we could split it up into small contiguous chunks and build a tree on top! he original useful data would be in the leaves, and each internal node would be a hash of everything below it. his would allow us to only recalculate a logarithmic number of hashes when something is modified. his data structure actually has a name, it's the famousnnerkle treenn.nnnfortunately, we still fall a bit short on the computational compleity. he above erkle tree layout is very efficient at incorporating modifications to eisting data, but insertions and deletions shift the chunk boundaries and invalidatennallnnthe calculated hashes.nnnstead of blindly chunking up the dataset, we could use the keys themselves to organize the data into a tree format based on common prefies! his way an insertion or deletion wouldn't shift all nodes, rather will change just the logarithmic path from root to leaf. his data structure is called annatricia treenn.nnombine the two ideas - the tree layout of a atricia tree and the hashing algorithm of a erkle tree - and you end up with annerkle atricia treenn, the actual data structure used to represent state in thereum. uaranteed logarithmic compleity for modifications, insertions, deletions and verification!nn tiny etra is that keys are hashed before insertion to balance the tries.nntate storage in thereumnnhe above description eplainsnnwhynnthereum stores its state in a erkle atricia tree. las, as fast as the desired operations got, every choice is a trade-off. he cost ofnnlogarithmic updates and logarithmic verificationnnisnnlogarithmic reads and logarithmic storagennfornnevery individual keynn. his is because every internal trie node needs to be saved to disk individually.nn do not have an accurate number for the depth of the account trie at the moment, but about a year ago we were saturating the depth of . his means, that every trie operation (e.g. read balance, write nonce) touches at least - internal nodes, thus will do at least - persistent database accesses. evel also organizes its data into a maimum of  levels, so there's an etra multiplier from there. he net result is that annsinglennstate access is epected to amplify intonn- randomnndisk accesses. ultiply this with all the state reads and writes that all the transactions in a block touch and you get to annscarynnnumber.nnf course all client implementations try their best to minimize this overhead. eth uses large memory areas for caching trie nodes and also uses in-memory pruning to avoid writing to disk nodes that get deleted anyway after a few blocks. hat's for a different blog post however.]nns horrible as these numbers are, these are the costs of operating an thereum node and having the capability of cryptograhically verifying all state at all times. ut can we do betternnot all accesses are created equalnnthereum relies on cryptographic proofs for its state. here is no way around the disk amplifications if we want to retain our capability to verify all the data. hat said, wenncan - and do -nntrust the data we've already verified.nnhere is no point to verify and re-verify every state item, every time we pull it up from disk. he erkle atricia tree is essential for writes, but it's an overhead for reads. e cannot get rid of it, and we cannot slim it down but thatnndoesn't meannnwe must necessarily use it everywhere.nnn thereum node accesses state in a few different placesnnhen importing a new block,  code eecution does a more-or-less balanced number of state reads and writes.  denial-of-service block might however do significantly more reads than writes.nnhen a node operator retrieves state (e.g.nneth_callnnand family),  code eecution only does reads (it can write too, but those get discarded at the end and are not persisted).nnhen a node is synchronizing, it is requesting state from remote nodes that need to dig it up and serve it over the network.nnased on the above access patterns, if we can short circuit reads not to hit the state trie, a slew of node operations will becomennsignificantlynnfaster. t might even enable some novel access patterns (like state iteration) which was prohibitively epensive before.nnf course, there's always a trade-off. ithout getting rid of the trie, any new acceleration structure is etra overhead. he question is whether the additional overhead provides enough value to warrant itnnack to the rootsnne've built this magical erkle atricia tree to solve all our problems, and now we want to get around it for reads. hat acceleration structure should we use to make reads fast again ell, if we don't need the trie, we don't need any of the compleity introduced. e can go all the way back to the origins.nns mentioned in the beginning of this post, thenntheoretical idealnndata storage for thereum's state is a simple key-value store (separate for accounts and each contract). ithout the constraints of the erkle atricia tree however, there's "nothing" stopping us from actually implementing the ideal solution!nn while back eth introduced itsnnsnapshotnnacceleration structure (not enabled by default).  snapshot is a complete view of the thereum state at a given block. bstract implementation wise, it is a dump of all accounts and storage slots, represented by a flat key-value store.nnhenever we wish to access an account or storage slot, we only pay  evel lookup instead of - as per the trie. pdating the snapshot is also simple in theory, after processing a block we do  etra evel write per updated slot.nnhe snapshot essentially reduces reads from (log n) to () (times evel overhead) at the cost of increasing writes from (log n) to ( + log n) (times evel overhead) and increasing disk storage from (n log n) to (n + n log n).nnevil's in the detailsnnaintaining a usable snapshot of the thereum state has its compleity. s long as blocks are coming one after the other, always building on top of the last, the naive approach of merging changes into the snapshot works. f there's a mini reorg however (even a single block), we're in trouble, because there's no undo. ersistent writes are one-way operation for a flat data representation. o make matters worse, accessing older state (e.g.  blocks old for some pp or + for fast/snap sync) is impossible.nno overcome this limitation, eth's snapshot is composed of two entities a persistent disk layer that is a complete snapshot of an older block (e.g. -) and a tree of in-memory diff layers that gather the writes on top.nnhenever a new block is processed, we do not merge the writes directly into the disk layer, rather just create a new in-memory diff layer with the changes. f enough in-memory diff layers are piled on top, the bottom ones start getting merged together and eventually pushed to disk. henever a state item is to be read, we start at the topmost diff layer and keep going backwards until we find it or reach the disk.nnhis data representation is very powerful as it solves a lot of issues. ince the in-memory diff layers are assembled into a tree, reorgs shallower than  blocks can simply pick the diff layer belonging to the parent block and build forward from there. pps and remote syncers needing older state have access to  recent ones. he cost does increase by  map lookups, but  in-memory lookups is orders of magnitude faster than  disk reads amplified - by evel.nnf course, there are lots and lots of gotchas and caveats. ithout going into too much details, a quick listing of the finer points arennelf-destructs (and deletions) are special beasts as they need to short circuit diff layer descent.nnf there is a reorg deeper than the persistent disk layer, the snapshot needs to be completely discarded and regenerated. his is very epensive.nnn shutdown, the in-memory diff layers need to be persisted into a journal and loaded back up, otherwise the snapshot will become useless on restart.nnse the bottom-most diff layer as an accumulator and only flush to disk when it eceeds some memory usage. his allows deduping writes for the same slots across blocks.nnllocate a read cache for the disk layer so that contracts accessing the same ancient slot over and over don't cause disk hits.nnse cumulative bloom filters in the in-memory diff layers to quickly detect whether there's a chance for an item to be in the diffs, or if we can go to disk immediately.nnhe keys are not the raw data (account address, storage key), rather the hashes of these, ensuring the snapshot has the same iteration order as the erkle atricia tree.nnenerating the persistent disk layer takes significantly more time than the pruning window for the state tries, so even the generator needs to dynamically follow the chain.nnhe good, the bad, the uglynneth's snapshot acceleration structure reduces state read compleity by about an order of magnitude. his means read-based o gets an order of magnitude harder to pull off andnneth_callnninvocations get an order of magnitude faster (if not  bound).nnhe snapshot also enables blazing fast state iteration of the most recent blocks.nnhis was actually the main reason for building snapshotsnn, as it permitted the creation of the newnnsnapnnsync algorithmnn. escribing that is an entirely new blog post, but the latest benchmarks on inkeby speak volumesnnf course, the trade-offs are always present. fter initial sync is complete, it takes about -h on mainnet to construct the initial snapshot (it's maintained live afterwards) and it takes about + of additional disk space.nns for the ugly part ell, it took us over  months to feel confident enough about the snapshot to ship it, but even now it's behind thenn--snapshotnnflag and there's still tuning and polishing to be done around memory usage and crash recovery.nnll in all, we're very proud of this improvement. t was an insane amount of work and it was a huge shot in the dark implementing everything and hoping it will work out. ust as a fun fact, the first version of snap sync (leaf sync) was written . years ago and was blocked ever since because we lacked the necessary acceleration to saturate it.nnpiloguennope you enjoyed this first post ofnnsk about ethnn. t took me about twice as much to finish it than  aimed for, but  felt the topic deserves the etra time. ee you net week.nn  deliberately didn't link the asking/voting website into this post as 'm sure it's a temporary thing and  don't want to leave broken links for posterity nor have someone buy the name and host something malicious in the future. ou can find itnnamong my witter postsnn.]