Titre: State Tree Pruning\nAuteur: Vitalik Buterin\nDate: June 26, 2015\nURL: https://blog.ethereum.org/2015/06/26/state-tree-pruning\nCatégorie: Non catégorisé\n\n==================================================\n\nne of the important issues that has been brought up over the course of the lympic stress-net release is the large amount of data that clients are required to store over little more than three months of operation, and particularly during the last month, the amount of data in each thereum client's blockchain folder has ballooned to an impressive - gigabytes, depending on which client you are using and whether or not compression is enabled. lthough it is important to note that this is indeed a stress test scenario where users are incentivized to dump transactions on the blockchain paying only the free test-ether as a transaction fee, and transaction throughput levels are thus several times higher than itcoin, it is nevertheless a legitimate concern for users, who in many cases do not have hundreds of gigabytes to spare on storing other people's transaction histories.nnirst of all, let us begin by eploring why the current thereum client database is so large. thereum, unlike itcoin, has the property that every block contains something called the "state root" the root hash of annspecialized kind of erkle treennwhich stores the entire state of the system all account balances, contract storage, contract code and account nonces are inside.nnhe purpose of this is simple it allows a node given only the last block, together with some assurance that the last block actually is the most recent block, to "synchronize" with the blockchain etremely quickly without processing any historical transactions, by simply downloading the rest of the tree from nodes in the network (the proposednnashookupnnwire protocol messagennwill faciliate this), verifying that the tree is correct by checking that all of the hashes match up, and then proceeding from there. n a fully decentralized contet, this will likely be done through an advanced version of itcoin's headers-first-verification strategy, which will look roughly as followsnnownload as many block headers as the client can get its hands on.nnetermine the header which is on the end of the longest chain. tarting from that header, go back  blocks for safety, and call the block at that position nnnn() ("the hundredth-generation grandparent of the head")nnownload the state tree from the state root of nnnn(), using thennashookupnnopcode (note that after the first one or two rounds, this can be parallelized among as many peers as desired). erify that all parts of the tree match up.nnroceed normally from there.nnor light clients, the state root is even more advantageous they can immediately determine the eact balance and status of any account by simply asking the network fornna particular branchnnof the tree, without needing to follow itcoin's multi-step -of- "ask for all transaction outputs, then ask for all transactions spending those outputs, and take the remainder" light-client model.nnowever, this state tree mechanism has an important disadvantage if implemented naively the intermediate nodes in the tree greatly increase the amount of disk space required to store all the data. o see why, consider this diagram herennhe change in the tree during each individual block is fairly small, and the magic of the tree as a data structure is that most of the data can simply be referenced twice without being copied. owever, even still, for every change to the state that is made, a logarithmically large number of nodes (ie. ~ at  nodes, ~ at  nodes, ~ at  nodes) need to be stored twice, one version for the old tree and one version for the new trie. ventually, as a node processes every block, we can thus epect the total disk space utilization to be, in computer science terms, roughlynn(n*log(n))nn, wherennnnnis the transaction load. n practical terms, the thereum blockchain is only . gigabytes, but the size of the database including all these etra nodes is - gigabytes.nno, what can we do ne backward-looking fi is to simply go ahead and implement headers-first syncing, essentially resetting new users' hard disk consumption to zero, and allowing users to keep their hard disk consumption low by re-syncing every one or two months, but that is a somewhat ugly solution. he alternative approach is to implementnnstate tree pruningnn essentially, usennreference countingnnto track when nodes in the tree (here using "node" in the computer-science term meaning "piece of data that is somewhere in a graph or tree structure", not "computer on the network") drop out of the tree, and at that point put them on "death row" unless the node somehow becomes used again within the netnnnnblocks (eg.nn  nn), after that number of blocks pass the node should be permanently deleted from the database. ssentially, we store the tree nodes that are part of the current state, and we even store recent history, but we do not store history older than  blocks.nnnnshould be set as low as possible to conserve space, but settingnnnntoo low compromises robustness once this technique is implemented, a node cannot revert back more thannnnnblocks without essentially completely restarting synchronization. ow, let's see how this approach can be implemented fully, taking into account all of the corner casesnnhen processing a block with numbernnnn, keep track of all nodes (in the state, tree and receipt trees) whose reference count drops to zero. lace the hashes of these nodes into a "death row" database in some kind of data structure so that the list can later be recalled by block number (specifically, block numbernn + nn), and mark the node database entry itself as being deletion-worthy at blocknn + nn.nnf a node that is on death row gets re-instated (a practical eample of this is account  acquiring some particular balance/nonce/code/storage combinationnnfnn, then switching to a different valuenngnn, and then account  acquiring statennfnnwhile the node fornnfnnis on death row), then increase its reference count back to one. f that node is deleted again at some future blocknnnn(withnn  nn), then put it back on the future block's death row to be deleted at blocknn + nn.nnhen you get to processing blocknn + nn, recall the list of hashes that you logged back during blocknnnn. heck the node associated with each hash if the node is still marked for deletionnnduring that specific blocknn(ie. not reinstated, and importantly not reinstated and then re-marked for deletionnnlaternn), delete it. elete the list of hashes in the death row database as well.nnometimes, the new head of a chain will not be on top of the previous head and you will need to revert a block. or these cases, you will need to keep in the database a journal of all changes to reference counts (that's "journal" as innnjournaling file systemsnn essentially an ordered list of the changes made) when reverting a block, delete the death row list generated when producing that block, and undo the changes made according to the journal (and delete the journal when you're done).nnhen processing a block, delete the journal at blocknn - nn you are not capable of reverting more thannnnnblocks anyway, so the journal is superfluous (and, if kept, would in fact defeat the whole point of pruning).nnnce this is done, the database should only be storing state nodes associated with the lastnnnnblocks, so you will still have all the information you need from those blocks but nothing more. n top of this, there are further optimizations. articularly, afternnnnblocks, transaction and receipt trees should be deleted entirely, and even blocks may arguably be deleted as well - although there is an important argument for keeping some subset of "archive nodes" that store absolutely everything so as to help the rest of the network acquire the data that it needs.nnow, how much savings can this give us s it turns out, quite a lot! articularly, if we were to take the ultimate daredevil route and gonn  nn(ie. lose absolutely all ability to handle even single-block forks, storing no history whatsoever), then the size of the database would essentially be the size of the state a value which, even now (this data was grabbed at block ) stands at roughly  megabytes - the majority of which is made up ofnnaccounts like this onennwith storage slots filled to deliberately spam the network. tnn  nn, we would get essentially the current size of - gigabytes, as most of the growth happened in the last hundred thousand blocks, and the etra space required for storing journals and death row lists would make up the rest of the difference. t every value in between, we can epect the disk space growth to be linear (ie.nn  nnwould take us about ninety percent of the way there to near-zero).nnote that we may want to pursue a hybrid strategy keeping everynnblocknnbut not everynnstate tree nodenn in this case, we would need to add roughly . gigabytes to store the block data. t's important to note that the cause of the blockchain size is  fast block times currently, the block headers of the last three months make up roughly  megabytes, and the rest is transactions of the last one month, so at high levels of usage we can epect to continue to see transactions dominate. hat said, light clients will also need to prune block headers if they are to survive in low-memory circumstances.nnhe strategy described above has been implemented in a very early alpha form innnpyethnn it will be implemented properly in all clients in due time after rontier launches, as such storage bloat is only a medium-term and not a short-term scalability concern.