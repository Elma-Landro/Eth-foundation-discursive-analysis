Titre: The 1.x Files: The State of Stateless Ethereum\nAuteur: Griffin Ichiba Hotchkiss\nDate: December 30, 2019\nURL: https://blog.ethereum.org/2019/12/30/eth1x-files-state-of-stateless-ethereum\nCatégorie: Non catégorisé\n\n==================================================\n\nn thennlast edition of he . filesnn, we did a quick re-cap of where the th . research initiative came from, what's at stake, and what some possible solutions are. e ended with the concept ofnnstateless ethereumnn, and left a more detailed eamination of the stateless client for this post.nntatelessnnis the new direction of th . research, so we're going to do a pretty deep dive and get a real sense of the challenges and possibilities that are epected on the road ahead. or those that want to dive even deeper, 'll do my best to link to more verbose resources whenever possible.nnhe tate of tateless thereumnno see where we're going, we must first understand where we are with the concept of 'state'. hen we say 'state', it's in the sense of "a state of affairs".nnhe complete ‘state’ of thereum describes the current status of all accounts and balances, as well as the collective memories of all smart contracts deployed and running in the . very finalized block in the chain has one and only one state, which is agreed upon by all participants in the network. hat state is changed and updated with each new block that is added to the chain.nnn the contet of th . research, it's important not just to know what statennisnn, butnnhow it's representednnin both the protocol (as defined in the yellow paper), and in most client implementations (e.g. geth, parity, trinity, besu, etc.).nnive it a triennhe data structure used in thereum is called a erkle-atricia rie. un fact 'rie' is originally taken from the word 'retrieval', but most people pronounce it as 'try' to distinguish it from 'tree' when speaking. ut  digress. hat we need to know about erkle-atricia ries is as followsnnt one end of the trie, there are all of the particular pieces of data that describe state (value nodes). his could be a particular account's balance, or a variable stored in a smart contract (such as the total supply of an - token). n the middle arennbranch nodesnn, which link all of the values together through hashing.  branch node is an array containing the hashes of its child nodes, and each branch node is subsequently hashed and put into the array of its parent node. his successive hashing eventually arrives at a single state root node on the other end of the trie.nnn the simplified diagram above, we can see each value, as well as thennpathnnthat describes how to get to that value. or eample, to get to -, we traverse the path ,,,. imilarly, - can be reached by traversing the path ,,,. ote that paths in this eample are always  characters in length, and that there is often only one path to take to reach a value.nnhis structure has the important property of being deterministic and cryptographically verifiable he only way to generate a state root is by computing it from each individual piece of the state, and two states that are identical can be easily proven so by comparing the root hash and the hashes that led to it (nna erkle proofnn). onversely, there is no way to create two different states with the same root hash, and any attempt to modify state with different values will result in a different state root hash.nnthereum optimizes the trie structure by introducing a few new node types that improve efficiency etension nodes and leaf nodes. hese encode parts of thennpathnninto nodes so that the trie is more compact.nnn this modified erkle-atricia trie structure, each node will lead to a choice between multiple net nodes, a compressed part of a path that subsequent nodes share, or values (prepended by the rest of their path, if necessary). t's the same data and the same organization, but this trie only needs  nodes instead of . hisnnseemsnnmore efficient, but with the benefit of hindsight, isn't actually optimal. e'll eplore why in the net section.nno arrive at a particular part of state (such as an account's current balance of ther), one needs to start at the state root and crawl along the trie from node to node until the desired value is reached. t each node, characters in thennpathnnare used to decide which net node to travel to, like anndivining rodnn, but for navigating hashed data structures.nnn the 'real' version used by thereum,nnpathsnnare the hashes of an address  characters ( bits) in length, andnnvaluesnnarenn-encoded datann. ranch nodes are arrays that contain  elements (siteen for each of the possible headecimal characters, and one for a value), while leaf nodes and etension nodes contain  elements (one partial path and either a value or the hash of the net child node). he thereum wiki is likely the best place tonnread more about thisnn, or, if you would like to get way into the weeds,nnthis articlennhas a great (but unfortunately deprecated)  trie eercise in ython to play with.nntick it in a atabasennt this point we should remind ourselves that the trie structure is just an abstract concept. t's a way of packing the totality of thereum state into one unified structure. hat structure, however, then needs to bennimplementednnin the code of the client, and stored on a disk (or a few thousand of them scattered around the globe). his means taking a multi-dimensional trie and stuffing it into an ordinary database, which understands onlynnkey, value]nnpairs.nnn most thereum clients (all ecept turbo-geth), the erkle-atricia rie is implemented by creating a distinctnnkey, value]nnpair for each node, where the value is the node itself, and the key is the hash of that node.nnhe process of traversing the trie, then, is more or less the same as the theoretical process described earlier. o look up an account balance, we would start with the root hash, and look up its value in the database to get the first branch node. sing the first character of our hashed address, we find the hash of the first node. e look that hash up in the database, and get our second node. sing the net character of the hashed address, we find the hash of the third node. f we're lucky, we might find an etension or leaf node along the way, and not need to go through all  nibbles -- but eventually, we'll arrive at our desired account, and be able to retrieve its balance from the database.nnomputing the hash of each new block is largely the same process, but in reverse tarting with all the edge nodes (accounts), the trie is built through successive hashings, until finally a new root hash is built and compared with the last agreed-upon block in the chain.nnere's where that bit about thennapparentnnefficiency of the state trie comes into play re-building the whole trie is very intensive on disk, and the modified erkle-atricia trie structure used by thereum is more protocol efficientnnat the cost of implementation efficiencynn. hose etra node types, leaf and etension, theoretically save on memory needed to store the trie, but they make the algorithms thatnnmodifynnthe state inside the regular database more comple. f course, a decently powerful computer can perform the process at blazing speed. heer processing power, however, only goes so far.nnync, baby, syncnno far we've limited our scope to what's going on in annnindividualnncomputer running an thereum implementation like geth. ut thereum is a network, and the whole point of all of this is to keep the same unified state consistent across thousands of computers worldwide, and between different implementations of the protocol.nnhe constantly shuffling tokens of #efi, cryptokitty auctions or cheeze wizard battles, and ordinary  transfers all combine to create a rapidly changing state for thereum clients to stay in sync with, and it gets harder and harder the more popular thereum becomes, and the deeper the state trie gets.nnurbo-geth is one implementation that gets to the root of the problem t flattens the trie database and uses the path of a node (rather than its hash) as the key, value] pair. his effectively makes the depth of the tree irrelevant for lookups, and allows for a variety of nifty features that can improve performance and reduce the load on disk when running a full node.nnhe thereum state isnnbignn, and it changes with every block. ow big, and how much of a change e can ballpark the current state of thereum at around  million nodes in the state trie. f these, about , (but as many as ,) need to be added or modified every  seconds. taying in sync with the thereum blockchain is, effectively, constantly building a new version of the state trie over and over again.nnhis multi-step process of state trie database operations is why thereum implementations are so taing on disk / and memory, and why even a "fast sync" can take up to  hours to complete, even on fast connections. o run a full node in thereum, a fast  (as opposed to a cheap, reliable ) is annrequirementnn, because processing state changes is etremely demanding on disk read/writes.nnere it's important to note that there is a very large and significant distinction betweennnestablishing a new node to syncnnandnnkeeping an eisting node syncednn--  distinction that, when we get to stateless thereum, will blur (hopefully).nnhe straightforward way to sync a node is with the "full sync" method tarting from the genesis block, a list of every transaction in each block is retrieved, and a state trie is built. ith each subsequent block, the state trie is modified, adding and modifying nodes as the complete history of the blockchain is replayed. t takes a full week to download and eecute a state change for every block from the beginning, but it's just a matter of time before the transactions you need are pending inclusion into the net new block, rather than being already solidified in an old one.nnnother method, aptly named "fast-sync", is quicker but more complicated  new client can, instead of requesting transactions from the beginning of time, request state entries from a recent, trusted 'checkpoint' block. t's far lessnntotalnninformation to download, but it is still a lot of information to process--nnsync is not currently limited by bandwidth, but by disk performancenn.nn fast-syncing node is essentially in a race with the tip of the chain. t needs to getnnallnnof the state at the 'checkpoint' before that state goes stale and stops being offered by full nodes (t can 'pivot' to a new checkpoint if that happens). nce a fast-syncing node overcomes the hurdle and get its state fully caught up with a checkpoint, it can then switch to full sync -- building and updating its own copy of state from the included transactions in each block.nnan  get a block witnessnne can now start to unpack the concept of stateless thereum. ne of the main goals is to makennnew nodesnnless painful to spin up. iven that only .% of the state is changing from block to block, it seems like there should be a means of cutting down on all that etra 'stuff' that needs to be downloaded before the full sync switchover.nnut this is one of the challenges imposed by thereum's cryptographically secure data structure n a trie, a change to just one value will result in a completely different root hash. hat's a feature, not a bug! t keeps everybody certain that they are on the same page (at the same state) with everyone else on the network.nno take a shortcut, we need a new piece of information about state a block witness.nnuppose that just one value in this trie has changed recently (highlighted in green)nn full node syncing the state (including this transaction) will go about it the old-fashioned way y taking all the pieces of state, and hashing them together to create a new root hash. hey can then easily verify that their state is the same as everyone else's (since they have the same hash, and the same history of transactions).nnut what about someone that has just tuned in hat's thennsmallestnnamount of information that new node needs in order to verify that -- at least for as long as it's been watching -- its observations are consistent with everyone elsesnn new, oblivious node will need older, wiser full nodes to providennproofnnthat the observed transaction fits in with everything they've seen so far about the state.nnn very abstract terms, a block witness proof provides all of the missing hashes in a state trie, combined with some 'structural' information about where in the trie those hashes belong. his allows an 'oblivious' node to include the new transaction in its state, and to compute the new root hash locally -- without requiring them to download an entire copy of the state trie.nnhis is, in a nutshell, the idea behindnnbeam syncnn. ather than waiting to collect each node in the checkpoint trie, beam sync begins watching and trying to eecute transactions as they happen, requesting a witness with each block from a full node for the information it doesn't have. s more and more of the state is 'touched' by new transactions, the client can rely more and more on its own copy of state, which (in beam sync) will gradually fill in until it eventually switches over to full sync.nntatelessness is a spectrumnnith the introduction of a block witness, the concept of 'fully stateless' starts to get more defined. t the same time, it's where we start to run into open questions and problems with no obvious solution.nnn contrast to beam sync, anntruly statelessnnclient wouldnnnevernnkeep a copy of state it would only grab the latest transactions together with the witness, and have everything it needs to eecute the net block.nnou might see that, if thennentire networknnwere stateless, this could actually hold up forever-- witnesses for new blocks can be produced from the previous block. t'd be witnesses all the way down! t least, down to the last agreed upon 'state of affiars', and the first witness generated from that state. hat's a big, dramatic change to thereum not likely to win widespread support.nn less dramatic approach is to accommodate varying degrees of 'statefullness', and have a network in whichnnsomennnodes keep a full copy of the state and can serve everyone else fresh witnesses.nnull-state nodes would operate as before, but would additionally compute a witness and either attach it to a new block, or propagate it through a secondary network sub-protocol.nnartial-state nodes could keep a full state for just a short number of blocks, or perhaps just 'watch' the piece of state that they're interested in, and get the rest of the data that they need to verify blocks from witnesses. his would help infrastructure-running dapp developers immensely.nnero-state nodes, who by definition want to keep their clients running as light as possible, could rely entirely on witnesses to verify new blocks.nnetting this scheme to work might entail something like bittorrent-style chunking and swarming behavior, where witness fragments are propagated according to their need and best connections to other nodes with (complementary) partial state. r, it might involve working out an alternative implementation of the state trie more amenable to witness generation. his is stuff to investigate and prototype!nnor a much more in-depth analysis of what the trade-offs of stateful vs stateless nodes are, see leey khunov'snnhe shades of statefulnessnn.nnn important feature of the semi-stateless approach is that these changes don'tnnnecessarilynnimply big, hard-forking changes. hrough small, testable, and incremental improvements, it's possible to build out the stateless component of thereum into a complementary sub-protocol, or as a series of un-controversial s instead of a large 'leap-of-faith' upgrade.nnhe road(map) aheadnnhe elephant in the research room isnnwitness sizenn. rdinary blocks contain a header, and a list of transactions, and are on the order of  k. his is small enough to make the propagation of blocks quick relative to network latency and the  second block time.nnitnesses, however, need to contain the hashes of nodes both at the edges and deep inside the state trie. his means they are much, much bigger early numbers suggest on the order of  . onsequently, syncing a witness is much much slower relative to network latency and block time, which could be a problem.nnhe dilemma is akin to the difference between downloading a movie or streaming it f the network is too slow to keep up with the stream, downloading the full movie is the only workable option. f the network is much faster, the movie can be streamed with no problem. n the middle, you need more data to decide. hose with sub-par s will recognize the gravity of attempting to stream a friday night movie over a network that might not be up for the task.nnhis, largely, is where we start getting into the detailed problems that the th  group is tackling. ight now, not enough is known about the hypothetical witness network to know for sure it'll work properly or optimally, but the devil is in the details (and the data).nnne line of inquiry is to think about ways to compress and reduce the size of witnesses by changing the structure of the trie itself (such as a binary trie), to make it more efficient at the implimentation level. nother is to prototype the network primitives (bittorrent-style swarming) that allow witnesses to be efficiently passed around between different nodes on the network. oth of these would benefit from a formalized witness specification -- which doesn't eist yet.nnll of these directions (and more) are being compiled into a more organized roadmap, which will be distilled and published in the coming weeks. he points highlighted on the roadmap will be topics of future deep dives.nnf you've made it this far, you should have a good idea of what "tateless thereum" is all about, and some of the contet for emerging th &.nns always, if you have questions about th efforts, requests for topics, or want to contribute, come introduce yourself on ethresear.ch or reach out to gichiba and/or ancock on twitter.nnpecial thanks to leey khunov for providing technical feedback and some of the trie diagrams.nnappy new year, and happy uir lacier hardfork!